model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-4
    litellm_params:
      model: gpt-4
      api_key: os.environ/OPENAI_API_KEY


# General settings
general_settings: 
  completion_model: gpt-3.5-turbo  # default model for /chat/completions
  embedding_model: text-embedding-ada-002  # default model for /embeddings
  max_tokens: 2000  # default max tokens for responses
  request_timeout: 600  # timeout for requests in seconds
  drop_params: false  # whether to drop unsupported params

# Key management
key_management:
  max_parallel_requests: 100  # maximum parallel requests per API key
  retry_attempts: 3  # number of retry attempts for failed requests
  timeout_seconds: 30  # timeout for each request attempt

# Routing rules
routing:
  - model: gpt-3.5-turbo
    max_tokens: 4096
    temperature: 0.7
    frequency_penalty: 0.0
    presence_penalty: 0.0

  - model: gpt-4
    max_tokens: 8192
    temperature: 0.7
    frequency_penalty: 0.0
    presence_penalty: 0.0

# Logging configuration
logging:
  level: info  # debug, info, warning, error
  log_requests: true
  log_responses: false  # set to true to log complete responses
  log_file: proxy_logs.log  # file to write logs to

# Health check settings
health_check:
  enabled: true
  interval: 60  # seconds between health checks
  timeout: 10  # seconds before health check times out

# Rate limiting
rate_limits:
  enabled: true
  default_rpm: 60  # requests per minute
  default_tpm: 100000  # tokens per minute
  max_parallel_requests: 100 

litellm_settings:
  callbacks: maxim_proxy_tracer.litellm_handler