{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ElevenLabs STT-TTS Integration with Maxim SDK\n",
        "\n",
        "This notebook demonstrates how to integrate Maxim SDK tracing with ElevenLabs Speech-to-Text (STT) and Text-to-Speech (TTS) operations.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The integration enables you to:\n",
        "- Trace STT operations (speech → text)\n",
        "- Trace TTS operations (text → speech)\n",
        "- Link both operations under a single trace\n",
        "- Attach audio files (input and output) to traces\n",
        "- Monitor the complete voice pipeline in Maxim dashboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "ELEVENLABS_API_KEY = os.getenv(\"EL_API_KEY\")\n",
        "\n",
        "if not ELEVENLABS_API_KEY:\n",
        "    raise ValueError(\"ELEVENLABS_API_KEY environment variable is not set\")\n",
        "\n",
        "print(\"✅ Environment variables loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize Maxim SDK and Instrument ElevenLabs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from maxim import Maxim\n",
        "from maxim.logger.elevenlabs import instrument_elevenlabs\n",
        "\n",
        "# Initialize Maxim logger\n",
        "# This automatically picks up MAXIM_API_KEY and MAXIM_LOG_REPO_ID from environment variables\n",
        "logger = Maxim().logger()\n",
        "\n",
        "# Instrument ElevenLabs STT/TTS methods (one-line integration)\n",
        "instrument_elevenlabs(logger)\n",
        "\n",
        "print(\"✅ Maxim SDK initialized and ElevenLabs instrumented\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Initialize ElevenLabs Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from elevenlabs.client import ElevenLabs\n",
        "\n",
        "# Initialize ElevenLabs client\n",
        "client = ElevenLabs(api_key=ELEVENLABS_API_KEY)\n",
        "\n",
        "print(\"✅ ElevenLabs client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create a Unified Trace for STT-TTS Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "from maxim.logger.components.trace import TraceConfigDict\n",
        "from elevenlabs.core import RequestOptions\n",
        "\n",
        "# Create a shared trace ID for the entire pipeline\n",
        "trace_id = str(uuid4())\n",
        "\n",
        "trace = logger.trace(\n",
        "    TraceConfigDict(\n",
        "        id=trace_id,\n",
        "        name=\"STT-TTS Pipeline Agent\",\n",
        "        tags={\"provider\": \"elevenlabs\", \"operation\": \"pipeline\"},\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create request options with trace_id header for both STT and TTS\n",
        "request_options = RequestOptions(\n",
        "    additional_headers={\n",
        "        \"x-maxim-trace-id\": trace_id\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"✅ Trace created with ID: {trace_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Speech-to-Text (STT)\n",
        "\n",
        "Convert audio input to text. The instrumentation automatically:\n",
        "- Adds the audio file as an input attachment\n",
        "- Sets the transcript as the trace input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Use sample audio file if available\n",
        "audio_file_path = os.path.join(\"files\", \"sample_audio.wav\")\n",
        "\n",
        "if os.path.exists(audio_file_path):\n",
        "    print(f\"Processing audio file: {audio_file_path}\")\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        transcript = client.speech_to_text.convert(\n",
        "            file=audio_file,\n",
        "            model_id=\"scribe_v1\",\n",
        "            request_options=request_options\n",
        "        )\n",
        "    \n",
        "    # Extract transcript text\n",
        "    if isinstance(transcript, str):\n",
        "        transcript_text = transcript\n",
        "    elif hasattr(transcript, \"text\"):\n",
        "        transcript_text = transcript.text\n",
        "    elif isinstance(transcript, dict) and \"text\" in transcript:\n",
        "        transcript_text = transcript[\"text\"]\n",
        "    else:\n",
        "        transcript_text = str(transcript)\n",
        "    \n",
        "    print(f\"✅ Transcript: {transcript_text}\")\n",
        "else:\n",
        "    # Option 2: Use dummy transcript for testing\n",
        "    print(\"Sample audio file not found. Using dummy transcript for demonstration.\")\n",
        "    transcript_text = \"Hello, how are you?\"\n",
        "    trace.set_input(transcript_text)\n",
        "    print(f\"✅ Using dummy transcript: {transcript_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Mock LLM Processing\n",
        "\n",
        "In a real scenario, this would call an actual LLM API to process the transcript.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mock_llm(transcript: str) -> str:\n",
        "    \"\"\"Mock LLM that generates a response based on the user's transcript.\"\"\"\n",
        "    transcript_lower = transcript.lower()\n",
        "    \n",
        "    if \"hello\" in transcript_lower or \"hi\" in transcript_lower:\n",
        "        return \"Hello! How can I help you today?\"\n",
        "    elif \"weather\" in transcript_lower:\n",
        "        return \"I'm sorry, I don't have access to weather information right now.\"\n",
        "    elif \"time\" in transcript_lower:\n",
        "        return \"I don't have access to the current time, but I'm here to help with other questions!\"\n",
        "    elif \"goodbye\" in transcript_lower or \"bye\" in transcript_lower:\n",
        "        return \"Goodbye! Have a great day!\"\n",
        "    else:\n",
        "        return f\"I heard you say: {transcript}. How can I assist you further?\"\n",
        "\n",
        "# Process transcript with mock LLM\n",
        "response_text = mock_llm(transcript_text)\n",
        "print(f\"✅ LLM Response: {response_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Text-to-Speech (TTS)\n",
        "\n",
        "Convert LLM response to audio. The instrumentation automatically:\n",
        "- Sets the response text as the trace output\n",
        "- Adds the generated audio file as an output attachment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from elevenlabs.play import play\n",
        "\n",
        "# Convert LLM response text to speech\n",
        "audio_output = client.text_to_speech.convert(\n",
        "    text=response_text,\n",
        "    voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n",
        "    model_id=\"eleven_multilingual_v2\",\n",
        "    output_format=\"mp3_44100_128\",\n",
        "    request_options=request_options\n",
        ")\n",
        "\n",
        "print(\"✅ Audio generated successfully\")\n",
        "print(\"Playing audio...\")\n",
        "play(audio_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Complete the Trace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trace.end()\n",
        "\n",
        "print(\"\\n=== Pipeline Complete ===\")\n",
        "print(\"Check your Maxim dashboard to see the unified trace with:\")\n",
        "print(\"- Input: User speech transcript (set by STT instrumentation)\")\n",
        "print(\"- Output: LLM response text (set by TTS instrumentation)\")\n",
        "print(\"- Input attachment: User speech audio file (added by STT instrumentation)\")\n",
        "print(\"- Output attachment: Assistant speech audio file (added by TTS instrumentation)\")\n",
        "print(f\"- Trace ID: {trace_id}\")\n",
        "\n",
        "# Cleanup\n",
        "logger.cleanup()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
