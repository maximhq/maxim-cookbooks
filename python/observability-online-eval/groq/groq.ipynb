{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Required Dependencies (groq, maxim-py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuyiYQ9BB2i_",
        "outputId": "3a8a10b2-782f-4908-edb9-c84cd5f4dfcc"
      },
      "outputs": [],
      "source": [
        "# !pip install groq\n",
        "# !pip install maxim-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ifbwECPIB5_O"
      },
      "outputs": [],
      "source": [
        "from groq import Groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ECs9RZXlE4sp"
      },
      "outputs": [],
      "source": [
        "from maxim.logger.groq import instrument_groq\n",
        "from maxim import Config, Maxim\n",
        "from maxim import logger\n",
        "from maxim.logger import LoggerConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRmRYFqSFRDe"
      },
      "source": [
        "## Create a Maxim Logger & Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnoyfPMAFQKf",
        "outputId": "85ed1cb4-b551-43a0-c361-468cbe2bea20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[MaximSDK] Initializing Maxim AI(v3.9.12)\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-5-308030384.py:1: DeprecationWarning: This class will be removed in a future version. Use {} which is TypedDict.\n",
            "  maxim = Maxim(Config(api_key= MAXIM_API_KEY))\n",
            "/tmp/ipython-input-5-308030384.py:3: DeprecationWarning: This class will be removed in a future version. Use LoggerConfigDict instead.\n",
            "  logger = maxim.logger(LoggerConfig(id=MAXIM_LOG_REPO_ID))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "maxim_api_key = os.environ.get(\"MAXIM_API_KEY\")\n",
        "maxim_log_repo_id = os.environ.get(\"MAXIM_LOG_REPO_ID\")\n",
        "\n",
        "maxim = Maxim(Config(api_key=maxim_api_key))\n",
        "\n",
        "logger = maxim.logger(LoggerConfig(id=maxim_log_repo_id))\n",
        "\n",
        "instrument_groq(logger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgQOUb3cDrN8"
      },
      "source": [
        "## Simple Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QoOyXU64B_M7"
      },
      "outputs": [],
      "source": [
        "client = Groq()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fvWbpCueCQNP"
      },
      "outputs": [],
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    model=\"llama-3.3-70b-versatile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOoTM_6eDisw",
        "outputId": "f613e90c-ebef-4fa8-c127-66bf3ad7acb0"
      },
      "outputs": [],
      "source": [
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaxo0GVzDo0b"
      },
      "source": [
        "## Streaming Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-AAjJxdDjrE",
        "outputId": "382b8813-2cfe-46ba-ea35-630ffd296ffd"
      },
      "outputs": [],
      "source": [
        "stream = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "    temperature=0.5,\n",
        "\n",
        "    max_completion_tokens=1024,\n",
        "\n",
        "    top_p=1,\n",
        "\n",
        "    stop=None,\n",
        "\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBQwao4qD6hv"
      },
      "source": [
        "## Async Chat Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd9oTtWsEJeL"
      },
      "outputs": [],
      "source": [
        "from groq import AsyncGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvQYxpVDzNE",
        "outputId": "c56a800d-3360-4130-ffec-7c3b656301c2"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "from groq import AsyncGroq\n",
        "\n",
        "async def main():\n",
        "    client = AsyncGroq()\n",
        "\n",
        "    chat_completion = await client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "            # Set a user message for the assistant to respond to.\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Explain the importance of fast language models\",\n",
        "            }\n",
        "        ],\n",
        "\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "        temperature=0.5,\n",
        "\n",
        "        max_completion_tokens=1024,\n",
        "        top_p=1,\n",
        "\n",
        "        stop=None,\n",
        "\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    # Print the completion returned by the LLM.\n",
        "    print(chat_completion.choices[0].message.content)\n",
        "\n",
        "await main() # Use asyncio.run if not working in jupyter environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2TW14ynEetv"
      },
      "source": [
        "## Async Completion with Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8e9kOAGEQlC",
        "outputId": "20ad1161-ae65-4aa1-d751-00c0de4d748f"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    client = AsyncGroq()\n",
        "\n",
        "    stream = await client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "            # Set a user message for the assistant to respond to.\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Explain the importance of fast language models\",\n",
        "            }\n",
        "        ],\n",
        "\n",
        "        # The language model which will generate the completion.\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "        temperature=0.5,\n",
        "\n",
        "        max_completion_tokens=1024,\n",
        "\n",
        "        top_p=1,\n",
        "\n",
        "        stop=None,\n",
        "\n",
        "        # If set, partial message deltas will be sent.\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    # Print the incremental deltas returned by the LLM.\n",
        "    async for chunk in stream:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
