{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuyiYQ9BB2i_",
        "outputId": "3a8a10b2-782f-4908-edb9-c84cd5f4dfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.30.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Requirement already satisfied: maxim-py in /usr/local/lib/python3.11/dist-packages (3.9.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from maxim-py) (4.14.1)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from maxim-py) (1.2.0)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from maxim-py) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->maxim-py) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->maxim-py) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->maxim-py) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->maxim-py) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->maxim-py) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->maxim-py) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "!pip install maxim-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq"
      ],
      "metadata": {
        "id": "ifbwECPIB5_O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from maxim.logger.groq import instrument_groq\n",
        "from maxim import Config, Maxim\n",
        "from maxim import logger\n",
        "from maxim.logger import LoggerConfig"
      ],
      "metadata": {
        "id": "ECs9RZXlE4sp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "MAXIM_API_KEY=userdata.get(\"MAXIM_API_KEY\")\n",
        "MAXIM_LOG_REPO_ID=userdata.get(\"MAXIM_REPO_ID\")\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "os.environ[\"MAXIM_API_KEY\"] = MAXIM_API_KEY\n",
        "os.environ[\"MAXIM_LOG_REPO_ID\"] = MAXIM_LOG_REPO_ID\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
      ],
      "metadata": {
        "id": "TBjH8DVUCelY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Maxim Logger & Callback"
      ],
      "metadata": {
        "id": "uRmRYFqSFRDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxim = Maxim(Config(api_key= MAXIM_API_KEY))\n",
        "\n",
        "logger = maxim.logger(LoggerConfig(id=MAXIM_LOG_REPO_ID))\n",
        "\n",
        "instrument_groq(logger)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnoyfPMAFQKf",
        "outputId": "85ed1cb4-b551-43a0-c361-468cbe2bea20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[MaximSDK] Initializing Maxim AI(v3.9.12)\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-308030384.py:1: DeprecationWarning: This class will be removed in a future version. Use {} which is TypedDict.\n",
            "  maxim = Maxim(Config(api_key= MAXIM_API_KEY))\n",
            "/tmp/ipython-input-5-308030384.py:3: DeprecationWarning: This class will be removed in a future version. Use LoggerConfigDict instead.\n",
            "  logger = maxim.logger(LoggerConfig(id=MAXIM_LOG_REPO_ID))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Inference"
      ],
      "metadata": {
        "id": "sgQOUb3cDrN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq()"
      ],
      "metadata": {
        "id": "QoOyXU64B_M7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    model=\"llama-3.3-70b-versatile\"\n",
        ")"
      ],
      "metadata": {
        "id": "fvWbpCueCQNP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOoTM_6eDisw",
        "outputId": "f613e90c-ebef-4fa8-c127-66bf3ad7acb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in various applications, including natural language processing (NLP), machine translation, text summarization, and chatbots. The importance of fast language models can be understood from several perspectives:\n",
            "\n",
            "1. **Real-time Applications**: Many NLP applications require rapid response times, such as virtual assistants, chatbots, and real-time translation systems. Fast language models enable these applications to process and respond to user input quickly, providing a seamless user experience.\n",
            "2. **Efficient Processing**: Fast language models can handle large volumes of text data, making them ideal for applications that involve processing vast amounts of unstructured data, such as text classification, sentiment analysis, and information retrieval.\n",
            "3. **Low Latency**: Fast language models minimize latency, which is critical in applications where timely responses are essential, such as in customer service chatbots, real-time translation systems, and emergency response systems.\n",
            "4. **Scalability**: Fast language models can be easily scaled up or down depending on the application requirements, making them suitable for a wide range of use cases, from small-scale chatbots to large-scale language translation systems.\n",
            "5. **Improved User Experience**: Fast language models enable applications to respond quickly to user input, improving the overall user experience and increasing user engagement.\n",
            "6. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage by providing faster and more accurate language-based services, such as language translation, text summarization, and sentiment analysis.\n",
            "7. **Cost Savings**: Fast language models can help reduce computational costs by minimizing the time and resources required to process and respond to user input, leading to cost savings and increased efficiency.\n",
            "8. **Enhanced Accuracy**: Fast language models can be trained on larger datasets and fine-tuned for specific tasks, leading to improved accuracy and better performance in various NLP applications.\n",
            "\n",
            "Some examples of fast language models include:\n",
            "\n",
            "1. **Transformers**: Introduced in 2017, transformers have revolutionized the field of NLP by providing a fast and efficient architecture for sequence-to-sequence tasks.\n",
            "2. **BERT**: Developed by Google, BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that has achieved state-of-the-art results in various NLP tasks, including question answering, sentiment analysis, and text classification.\n",
            "3. **RoBERTa**: RoBERTa is a variant of BERT that has been optimized for speed and efficiency, making it suitable for real-time applications.\n",
            "\n",
            "In summary, fast language models are essential for a wide range of NLP applications, enabling real-time processing, efficient processing, low latency, scalability, and improved user experience, while also providing a competitive advantage, cost savings, and enhanced accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Response"
      ],
      "metadata": {
        "id": "eaxo0GVzDo0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "    temperature=0.5,\n",
        "\n",
        "    max_completion_tokens=1024,\n",
        "\n",
        "    top_p=1,\n",
        "\n",
        "    stop=None,\n",
        "\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-AAjJxdDjrE",
        "outputId": "382b8813-2cfe-46ba-ea35-630ffd296ffd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NoneFast language models are crucial in today's technological landscape, and their importance can be understood from several perspectives:\n",
            "\n",
            "1. **Efficient Processing**: Fast language models enable quick processing of vast amounts of text data. This efficiency is vital for applications where real-time or near-real-time responses are necessary, such as in chatbots, virtual assistants, and real-time translation services. Faster models can handle more requests and larger datasets without significant delays, making them indispensable for high-traffic and high-data-volume applications.\n",
            "\n",
            "2. **Resource Optimization**: Speed in language models often correlates with computational resource efficiency. Faster models typically require less computational power (e.g., GPU resources) to achieve the same or better results than slower models. This optimization is critical for reducing operational costs, especially in cloud-based services where computational resources are billed based on usage. Moreover, efficient models can run on less powerful hardware, making advanced language processing more accessible on a wider range of devices, including mobile and edge devices.\n",
            "\n",
            "3. **Enhanced User Experience**: The responsiveness of fast language models directly impacts user experience. In interactive applications, such as voice assistants, fast and accurate responses are key to maintaining user engagement and satisfaction. Delays in processing can lead to frustration and a perception of poor performance, even if the eventual response is accurate. Fast models ensure that users receive timely feedback, which is essential for natural, human-like interactions.\n",
            "\n",
            "4. **Competitive Advantage**: In the competitive landscape of technology and service provision, the ability to offer fast, reliable, and accurate language processing can be a significant differentiator. Companies that can provide quicker and more efficient language services can attract more users and maintain a competitive edge over slower, less efficient solutions.\n",
            "\n",
            "5. **Research and Development**: Fast language models facilitate quicker experimentation and iteration in research and development. Researchers can test hypotheses, train models, and evaluate results more rapidly, which accelerates the development of new technologies and applications based on language models. This speed is particularly beneficial in areas like natural language processing (NLP), where the complexity and size of models and datasets are continuously increasing.\n",
            "\n",
            "6. **Accessibility and Inclusion**: By enabling language processing on a wider range of devices and reducing the need for high-end hardware, fast language models can increase accessibility to advanced language technologies for diverse populations, including those in areas with less robust technological infrastructure. This accessibility can help bridge language and communication gaps globally, promoting inclusion and equity in the digital world.\n",
            "\n",
            "7. **Real-Time Applications**: Fast language models are essential for real-time applications such as live captioning, simultaneous translation, and emergency response systems, where timely and accurate information can be critical. The speed of these models ensures that critical information is conveyed quickly, which can be lifesaving or significantly impactful in high-stakes situations.\n",
            "\n",
            "In summary, the importance of fast language models lies in their ability to provide efficient, responsive, and accessible language processing, which is crucial for enhancing user experience, driving technological innovation, and ensuring competitiveness in today's fast-paced digital landscape.None"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Async Chat Completion"
      ],
      "metadata": {
        "id": "fBQwao4qD6hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from groq import AsyncGroq"
      ],
      "metadata": {
        "id": "pd9oTtWsEJeL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from groq import AsyncGroq\n",
        "\n",
        "async def main():\n",
        "    client = AsyncGroq()\n",
        "\n",
        "    chat_completion = await client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "            # Set a user message for the assistant to respond to.\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Explain the importance of fast language models\",\n",
        "            }\n",
        "        ],\n",
        "\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "        temperature=0.5,\n",
        "\n",
        "        max_completion_tokens=1024,\n",
        "        top_p=1,\n",
        "\n",
        "        stop=None,\n",
        "\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    # Print the completion returned by the LLM.\n",
        "    print(chat_completion.choices[0].message.content)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvQYxpVDzNE",
        "outputId": "c56a800d-3360-4130-ffec-7c3b656301c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in today's technology-driven world, and their importance can be understood from several perspectives:\n",
            "\n",
            "1. **Efficient Processing**: Fast language models can process and analyze vast amounts of text data quickly, making them essential for applications where speed is critical. This efficiency enables real-time language understanding, sentiment analysis, and text generation, which are vital in various industries such as customer service, social media monitoring, and content creation.\n",
            "\n",
            "2. **Improved User Experience**: The speed of language models directly impacts the user experience in applications like chatbots, virtual assistants, and language translation software. Faster models respond more quickly to user queries, providing a more seamless and interactive experience. This, in turn, enhances user engagement, satisfaction, and loyalty.\n",
            "\n",
            "3. **Scalability and Cost-Effectiveness**: Fast language models can handle a large volume of requests without significant increases in computational resources or costs. This scalability is vital for businesses and organizations that need to process massive amounts of text data or serve a large user base. By reducing the computational requirements and energy consumption, fast models contribute to cost savings and environmental sustainability.\n",
            "\n",
            "4. **Enhanced Accuracy and Reliability**: While speed is crucial, it must be balanced with accuracy. Fast language models that are also highly accurate enable reliable text analysis and generation. This is particularly important in critical applications such as medical diagnosis, legal document analysis, and financial reporting, where accuracy can have significant consequences.\n",
            "\n",
            "5. **Competitive Advantage**: In a competitive market, the ability to quickly develop, deploy, and improve language models can provide a significant competitive advantage. Companies that can leverage fast and accurate language models can innovate faster, respond more rapidly to market changes, and offer more sophisticated services and products to their customers.\n",
            "\n",
            "6. **Research and Development**: Fast language models facilitate faster experimentation and iteration in research and development. Scientists and engineers can quickly test hypotheses, evaluate model performance, and refine their approaches, leading to accelerated progress in natural language processing (NLP) and related fields.\n",
            "\n",
            "7. **Accessibility and Inclusion**: By enabling faster and more efficient language processing, these models can help bridge language gaps and make information more accessible to people worldwide. This is especially important for minority languages or regions with limited technological infrastructure, promoting global communication, education, and economic opportunities.\n",
            "\n",
            "In summary, fast language models are vital for enhancing efficiency, user experience, scalability, accuracy, competitiveness, research, and global accessibility. As technology continues to evolve, the importance of these models will only grow, driving innovation and transformation across various sectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Async Completion with Streaming"
      ],
      "metadata": {
        "id": "U2TW14ynEetv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from groq import AsyncGroq\n",
        "\n",
        "\n",
        "async def main():\n",
        "    client = AsyncGroq()\n",
        "\n",
        "    stream = await client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "            # Set a user message for the assistant to respond to.\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Explain the importance of fast language models\",\n",
        "            }\n",
        "        ],\n",
        "\n",
        "        # The language model which will generate the completion.\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "        temperature=0.5,\n",
        "\n",
        "        max_completion_tokens=1024,\n",
        "\n",
        "        top_p=1,\n",
        "\n",
        "        stop=None,\n",
        "\n",
        "        # If set, partial message deltas will be sent.\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    # Print the incremental deltas returned by the LLM.\n",
        "    async for chunk in stream:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8e9kOAGEQlC",
        "outputId": "20ad1161-ae65-4aa1-d751-00c0de4d748f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NoneFast language models are crucial in today's technology landscape, and their importance can be seen in several aspects:\n",
            "\n",
            "1. **Efficient Processing**: Fast language models can process and analyze large amounts of text data quickly, which is essential for applications that require real-time responses, such as chatbots, virtual assistants, and language translation software.\n",
            "2. **Improved User Experience**: Faster language models enable more responsive and interactive systems, leading to a better user experience. For example, a fast language model can quickly generate human-like responses to user queries, making the interaction feel more natural and engaging.\n",
            "3. **Scalability**: Fast language models can handle a large volume of requests and conversations simultaneously, making them ideal for large-scale applications, such as customer service chatbots, language translation platforms, and content generation tools.\n",
            "4. **Reduced Latency**: Fast language models minimize the time it takes to generate responses, which is critical in applications where timely responses are essential, such as in emergency services, healthcare, or financial transactions.\n",
            "5. **Increased Productivity**: By automating tasks that involve language processing, fast language models can free up human resources, allowing people to focus on higher-value tasks and increasing overall productivity.\n",
            "6. **Enhanced Accuracy**: Faster language models can process more data and iterate on their responses more quickly, which can lead to improved accuracy and better decision-making.\n",
            "7. **Cost Savings**: Fast language models can reduce the computational resources required to process language tasks, resulting in cost savings and making language-based applications more accessible to a wider range of users.\n",
            "8. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage by providing faster, more responsive, and more accurate language-based services, which can lead to increased customer satisfaction and loyalty.\n",
            "9. **Research and Development**: Fast language models can accelerate research and development in areas like natural language processing, machine learning, and artificial intelligence, leading to breakthroughs and innovations in these fields.\n",
            "10. **Accessibility**: Fast language models can enable more people to access language-based services, including those with disabilities, language barriers, or limited technical expertise, promoting inclusivity and equality.\n",
            "\n",
            "Some examples of applications that benefit from fast language models include:\n",
            "\n",
            "* Virtual assistants (e.g., Siri, Alexa, Google Assistant)\n",
            "* Chatbots and customer service platforms\n",
            "* Language translation software (e.g., Google Translate)\n",
            "* Content generation tools (e.g., text summarization, article writing)\n",
            "* Sentiment analysis and opinion mining\n",
            "* Speech recognition and transcription\n",
            "\n",
            "In summary, fast language models are essential for building efficient, scalable, and responsive language-based applications that can process and analyze large amounts of text data quickly, leading to improved user experiences, increased productivity, and cost savings.None"
          ]
        }
      ]
    }
  ]
}