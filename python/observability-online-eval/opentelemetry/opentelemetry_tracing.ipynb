{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenTelemetry with OpenAI API\n",
    "\n",
    "This notebook demonstrates how to use OpenTelemetry to trace OpenAI API calls.\n",
    "\n",
    "Learn more about OpenTelemetry ingestion using MaximAI at https://www.getmaxim.ai/docs/tracing/opentelemetry/ingesting-via-otlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install openai\n",
    "%pip install opentelemetry-api\n",
    "%pip install opentelemetry-exporter-otlp\n",
    "%pip install opentelemetry-exporter-otlp-proto-http\n",
    "%pip install opentelemetry-sdk\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry import trace\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n",
    "\n",
    "Rename `.env.example` to `.env` and fill in the following variables:\n",
    "- `MAXIM_API_KEY` - Your Maxim API key\n",
    "- `MAXIM_REPO_ID` - Your Maxim Log repository ID\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys and IDs from environment variables\n",
    "maxim_api_key = os.getenv(\"MAXIM_API_KEY\")\n",
    "repo_id = os.getenv(\"MAXIM_REPO_ID\")\n",
    "\n",
    "# Check if environment variables are set\n",
    "if not maxim_api_key or not repo_id:\n",
    "    print(\"Warning: MAXIM_API_KEY or MAXIM_REPO_ID environment variables are not set.\")\n",
    "    raise ValueError(\"MAXIM_API_KEY or MAXIM_REPO_ID environment variables are not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OpenTelemetry with Maxim OTLP ingest endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenTelemetry tracer provider\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "\n",
    "# Configure OTLP exporter\n",
    "span_exporter = OTLPSpanExporter(\n",
    "    endpoint=\"https://api.getmaxim.ai/v1/otel\",\n",
    "    headers={\n",
    "        \"x-maxim-api-key\": maxim_api_key,\n",
    "        \"x-maxim-repo-id\": repo_id,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add span processors\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter))\n",
    "\n",
    "# Set the global tracer provider\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "\n",
    "# Create a tracer\n",
    "tracer = trace.get_tracer(\"maxim_test_tracer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    print(\"Warning: OPENAI_API_KEY environment variable is not set.\")\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make OpenAI API Call with OpenTelemetry Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"genai_chat_completion\") as span:\n",
    "    # Set GenAI span attributes\n",
    "    span.set_attribute(\"gen_ai.system\", \"openai\")\n",
    "    span.set_attribute(\"gen_ai.request.model\", \"gpt-4o-mini\")\n",
    "    span.set_attribute(\"gen_ai.request.max_tokens\", 400)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant and you are going to answer in one sentence only.\"\n",
    "    )\n",
    "    user_message = \"What is LLM Observability?\"\n",
    "\n",
    "    # System message event\n",
    "    span.add_event(\n",
    "        \"gen_ai.system.message\",\n",
    "        attributes={\n",
    "            \"gen_ai.system\": \"openai\",\n",
    "            \"content\": system_message,\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # User message event\n",
    "    span.add_event(\n",
    "        \"gen_ai.user.message\",\n",
    "        attributes={\n",
    "            \"gen_ai.system\": \"openai\",\n",
    "            \"content\": user_message,\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Make the API call\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_message,\n",
    "            },\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_tokens=400,\n",
    "    )\n",
    "\n",
    "    # Set response attributes\n",
    "    span.set_attribute(\"gen_ai.response.id\", chat_completion.id)\n",
    "    span.set_attribute(\"gen_ai.response.model\", chat_completion.model)\n",
    "    span.set_attribute(\"gen_ai.usage.input_tokens\", chat_completion.usage.prompt_tokens)\n",
    "    span.set_attribute(\n",
    "        \"gen_ai.usage.output_tokens\", chat_completion.usage.completion_tokens\n",
    "    )\n",
    "\n",
    "    # Assistant message event (optional - captures the response)\n",
    "    span.add_event(\n",
    "        \"gen_ai.assistant.message\",\n",
    "        attributes={\n",
    "            \"gen_ai.system\": \"openai\",\n",
    "            \"content\": chat_completion.choices[0].message.content,\n",
    "            \"role\": \"assistant\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Choice events - one for each choice\n",
    "    for choice in chat_completion.choices:\n",
    "        choice_body = {\n",
    "            \"finish_reason\": choice.finish_reason,\n",
    "            \"index\": choice.index,\n",
    "            \"message\": {\n",
    "                \"content\": choice.message.content,\n",
    "                \"role\": choice.message.role,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Only include tool_calls if present\n",
    "        if choice.message.tool_calls:\n",
    "            choice_body[\"message\"][\"tool_calls\"] = [\n",
    "                {\n",
    "                    \"id\": tool_call.id,\n",
    "                    \"type\": tool_call.type,\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_call.function.name,\n",
    "                        \"arguments\": tool_call.function.arguments,\n",
    "                    },\n",
    "                }\n",
    "                for tool_call in choice.message.tool_calls\n",
    "            ]\n",
    "\n",
    "        span.add_event(\n",
    "            \"gen_ai.choice\",\n",
    "            attributes={\n",
    "                \"finish_reason\": choice_body[\"finish_reason\"],\n",
    "                \"index\": choice_body[\"index\"],\n",
    "                \"message\": json.dumps(choice_body[\"message\"]),\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the response\n",
    "print(\"Response from OpenAI:\")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
