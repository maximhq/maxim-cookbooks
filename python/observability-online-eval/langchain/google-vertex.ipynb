{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate Langchain + Gemini with Maxim tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Maxim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[MaximSDK] Initializing Maxim AI(v3.8.3)\u001b[0m\n",
      "\u001b[32m[MaximSDK] Using info logging level.\u001b[0m\n",
      "\u001b[32m[MaximSDK] For debug logs, set global logging level to debug logging.basicConfig(level=logging.DEBUG).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from maxim import Maxim\n",
    "\n",
    "logger = Maxim().logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke call with ChatVertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"maxim-development-433105\", location=\"us-west1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response generations=[[GenerationChunk(text=\"Okay! To give you the latest football news, I need to know what you're most interested in. Tell me, what kind of football news are you looking for? For example:\\n\\n*   **Which league/competition are you interested in?** (e.g., Premier League, Champions League, La Liga, NFL, etc.)\\n*   **Which team are you interested in?** (e.g., Manchester United, Real Madrid, etc.)\\n*   **What kind of news are you looking for?** (e.g., transfer rumors, match results, injury updates, manager news, etc.)\\n\\nOnce I have a better idea of what you're looking for, I can provide you with the most relevant information.\\n\", generation_info={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 154, 'total_token_count': 168, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 154}]}})]] llm_output=None run=None {'run_id': UUID('42747f9c-9e2c-43fb-a1c2-22943389701e'), 'parent_run_id': None, 'tags': []}\n",
      "[[GenerationChunk(text=\"Okay! To give you the latest football news, I need to know what you're most interested in. Tell me, what kind of football news are you looking for? For example:\\n\\n*   **Which league/competition are you interested in?** (e.g., Premier League, Champions League, La Liga, NFL, etc.)\\n*   **Which team are you interested in?** (e.g., Manchester United, Real Madrid, etc.)\\n*   **What kind of news are you looking for?** (e.g., transfer rumors, match results, injury updates, manager news, etc.)\\n\\nOnce I have a better idea of what you're looking for, I can provide you with the most relevant information.\\n\", generation_info={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 154, 'total_token_count': 168, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 154}]}})]]\n",
      "gen text=\"Okay! To give you the latest football news, I need to know what you're most interested in. Tell me, what kind of football news are you looking for? For example:\\n\\n*   **Which league/competition are you interested in?** (e.g., Premier League, Champions League, La Liga, NFL, etc.)\\n*   **Which team are you interested in?** (e.g., Manchester United, Real Madrid, etc.)\\n*   **What kind of news are you looking for?** (e.g., transfer rumors, match results, injury updates, manager news, etc.)\\n\\nOnce I have a better idea of what you're looking for, I can provide you with the most relevant information.\\n\" generation_info={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 154, 'total_token_count': 168, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 154}]}}\n",
      "generation_info {'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 154, 'total_token_count': 168, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 154}]}}\n",
      "usage_data {'prompt_token_count': 14, 'candidates_token_count': 154, 'total_token_count': 168, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 154}]}\n",
      "Okay! To give you the latest football news, I need to know what you're most interested in. Tell me, what kind of football news are you looking for? For example:\n",
      "\n",
      "*   **Which league/competition are you interested in?** (e.g., Premier League, Champions League, La Liga, NFL, etc.)\n",
      "*   **Which team are you interested in?** (e.g., Manchester United, Real Madrid, etc.)\n",
      "*   **What kind of news are you looking for?** (e.g., transfer rumors, match results, injury updates, manager news, etc.)\n",
      "\n",
      "Once I have a better idea of what you're looking for, I can provide you with the most relevant information.\n",
      "\n",
      "response generations=[[GenerationChunk(text=\"Okay! To give you the latest football news, I need to know what you're interested in.  Tell me:\\n\\n*   **Which football are you interested in?** (Soccer/Association Football, American Football, Australian Rules Football, etc.)\\n*   **If soccer, which leagues or teams are you following?** (e.g., Premier League, La Liga, Champions League, Manchester United, Real Madrid, etc.)\\n*   **Are you interested in specific news like scores, transfers, injuries, or something else?**\\n\\nOnce I have this information, I can give you a relevant update.\\n\", generation_info={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 128, 'total_token_count': 142, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 128}]}})]] llm_output=None run=None {'run_id': UUID('faea29a0-d678-48eb-a656-73799d6c117b'), 'parent_run_id': None, 'tags': []}\n",
      "[[GenerationChunk(text=\"Okay! To give you the latest football news, I need to know what you're interested in.  Tell me:\\n\\n*   **Which football are you interested in?** (Soccer/Association Football, American Football, Australian Rules Football, etc.)\\n*   **If soccer, which leagues or teams are you following?** (e.g., Premier League, La Liga, Champions League, Manchester United, Real Madrid, etc.)\\n*   **Are you interested in specific news like scores, transfers, injuries, or something else?**\\n\\nOnce I have this information, I can give you a relevant update.\\n\", generation_info={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 128, 'total_token_count': 142, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 128}]}})]]\n",
      "gen text=\"Okay! To give you the latest football news, I need to know what you're interested in.  Tell me:\\n\\n*   **Which football are you interested in?** (Soccer/Association Football, American Football, Australian Rules Football, etc.)\\n*   **If soccer, which leagues or teams are you following?** (e.g., Premier League, La Liga, Champions League, Manchester United, Real Madrid, etc.)\\n*   **Are you interested in specific news like scores, transfers, injuries, or something else?**\\n\\nOnce I have this information, I can give you a relevant update.\\n\" generation_info={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 128, 'total_token_count': 142, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 128}]}}\n",
      "generation_info {'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 14, 'candidates_token_count': 128, 'total_token_count': 142, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 128}]}}\n",
      "usage_data {'prompt_token_count': 14, 'candidates_token_count': 128, 'total_token_count': 142, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 14}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 128}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "from maxim.logger.langchain import MaximLangchainTracer\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "\n",
    "# To use model\n",
    "llm = VertexAI(model_name=\"gemini-2.0-flash-001\")\n",
    "\n",
    "langchain_tracer = MaximLangchainTracer(logger)\n",
    "\n",
    "user_input = \"latest football news\"\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "trace_id = str(uuid.uuid4())\n",
    "trace = logger.trace({\"id\": trace_id, \"name\": \"langchain_trace\"})\n",
    "trace.end()\n",
    "# Make the API call to Claude using LangChain\n",
    "messages = [(\"system\", system_message), (\"human\", user_input)]\n",
    "response = llm.invoke(messages,config={\"callbacks\":[langchain_tracer],\"metadata\":{\"maxim\":{\"trace_id\":trace.id}}})\n",
    "print(response)\n",
    "\n",
    "\n",
    "# service 2\n",
    "trace = logger.trace({\"id\": trace_id, \"name\": \"langchain_trace\"})\n",
    "span_id=str(uuid.uuid4())\n",
    "span = trace.span({\"id\": span_id,\"name\": \"langchain_span\"})\n",
    "\n",
    "response = llm.invoke(messages,config={\"callbacks\":[langchain_tracer],\"metadata\":{\"maxim\":{\"span_id\":span_id}}})\n",
    "span.event(str(uuid.uuid4()),\"langchain_event\")\n",
    "span.end()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response generations=[[GenerationChunk(text='{}', generation_info={'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 30, 'candidates_token_count': 1, 'total_token_count': 73, 'prompt_tokens_details': [{'modality': 'TEXT', 'token_count': 30}], 'candidates_tokens_details': [{'modality': 'TEXT', 'token_count': 1}], 'thoughts_token_count': 42}})]] llm_output=None run=None {'run_id': UUID('312589d3-8f21-43ff-b2c0-5ae964a74d1e'), 'parent_run_id': UUID('3e878b31-9e25-48b1-abbf-605f4e2b203d'), 'tags': ['seq:step:2']}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = VertexAI(model=\"gemini-2.5-flash-preview-05-20\",streaming=True,stream_usage=True)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful assistant.\"), (\"human\", \"{input}\")]\n",
    ")\n",
    "\n",
    "# Create a chain using pipe\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Run the chain with callbacks for tracing\n",
    "user_input = (\n",
    "    \"give me a basic json response. dont add anything else, just the json. not even ```\"\n",
    ")\n",
    "response = \"\"\n",
    "chunk_index = 0\n",
    "start_time = time.time()\n",
    "for event in chain.stream(\n",
    "    input=user_input,\n",
    "    config={\n",
    "        \"callbacks\": [langchain_tracer],\n",
    "        \"metadata\":{\"maxim\":{\"trace_tags\": {\"test\":\"test-123\"}}}\n",
    "    },\n",
    "):\n",
    "    if chunk_index == 0:\n",
    "        time_to_first_chunk = time.time() - start_time\n",
    "    response = json.dumps(event)\n",
    "    last_chunk = json.dumps(event)\n",
    "    chunk_index += 1\n",
    "time_to_last_chunk = time.time() - start_time\n",
    "logger.flush()\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
