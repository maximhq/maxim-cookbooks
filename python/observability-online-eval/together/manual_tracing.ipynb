{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Environment setup\n",
    "MAXIM_API_KEY = os.getenv(\"MAXIM_API_KEY\")\n",
    "LOG_REPOSITORY_ID = os.getenv(\"MAXIM_LOG_REPO_ID\")\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.2-8B\"\n",
    "\n",
    "if not all([MAXIM_API_KEY, LOG_REPOSITORY_ID, TOGETHER_API_KEY]):\n",
    "    raise EnvironmentError(\"API keys and Log Repository ID are not set in the environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from maxim import Maxim\n",
    "\n",
    "logger = Maxim().logger({\"id\": LOG_REPOSITORY_ID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = logger.session({\n",
    "    \"id\": str(uuid4()),\n",
    "})\n",
    "trace = session.trace({\n",
    "    \"id\": str(uuid4()),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "from maxim.logger import  GenerationError\n",
    "import time\n",
    "\n",
    "# Together AI client setup\n",
    "client = Together(api_key=TOGETHER_API_KEY)\n",
    "\n",
    "def get_together_response(messages):\n",
    "    generation_id = str(uuid4())\n",
    "    generation = trace.generation({\n",
    "        \"id\": generation_id,\n",
    "        \"name\": \"generation\",\n",
    "        \"provider\": \"together\",\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Get response from Together AI\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Log the generation result\n",
    "        generation.result({\n",
    "            \"id\": generation_id,\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time()),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"text\": response_text,\n",
    "                    \"logprobs\": None,\n",
    "                    \"finish_reason\": \"stop\",\n",
    "                }\n",
    "            ],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": getattr(response, 'usage', {}).get('prompt_tokens', 0),\n",
    "                \"completion_tokens\": getattr(response, 'usage', {}).get('completion_tokens', 0),\n",
    "                \"total_tokens\": getattr(response, 'usage', {}).get('total_tokens', 0),\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        generation.error(GenerationError(str(e)))\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        generation.end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Error code: 404 - {\"message\": \"Unable to access model meta-llama/Meta-Llama-3.2-8B. Please visit https://api.together.ai/models to view the list of supported models.\", \"type_\": \"invalid_request_error\", \"code\": \"model_not_available\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidRequestError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m trace.event(\u001b[38;5;28mstr\u001b[39m(uuid4()), \u001b[33m\"\u001b[39m\u001b[33mtest event\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Get and log response\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m completion = \u001b[43mget_together_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Print result\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTogether AI Response:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mget_together_response\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m     10\u001b[39m generation = trace.generation({\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: generation_id,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages\n\u001b[32m     16\u001b[39m })\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Get response from Together AI\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     response_text = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Log the generation result\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codebase/maxim-cookbooks/python/observability-online-eval/together/.venv/lib/python3.12/site-packages/together/resources/chat/completions.py:141\u001b[39m, in \u001b[36mChatCompletions.create\u001b[39m\u001b[34m(self, messages, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, response_format, tools, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m requestor = api_requestor.APIRequestor(\n\u001b[32m    113\u001b[39m     client=\u001b[38;5;28mself\u001b[39m._client,\n\u001b[32m    114\u001b[39m )\n\u001b[32m    116\u001b[39m parameter_payload = ChatCompletionRequest(\n\u001b[32m    117\u001b[39m     model=model,\n\u001b[32m    118\u001b[39m     messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     **kwargs,\n\u001b[32m    139\u001b[39m ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m response, _, _ = \u001b[43mrequestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codebase/maxim-cookbooks/python/observability-online-eval/together/.venv/lib/python3.12/site-packages/together/abstract/api_requestor.py:249\u001b[39m, in \u001b[36mAPIRequestor.request\u001b[39m\u001b[34m(self, options, stream, remaining_retries, request_timeout)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    233\u001b[39m     options: TogetherRequest,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    241\u001b[39m ]:\n\u001b[32m    242\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.request_raw(\n\u001b[32m    243\u001b[39m         options=options,\n\u001b[32m    244\u001b[39m         remaining_retries=remaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retries,\n\u001b[32m    245\u001b[39m         stream=stream,\n\u001b[32m    246\u001b[39m         request_timeout=request_timeout,\n\u001b[32m    247\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     resp, got_stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m.api_key\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codebase/maxim-cookbooks/python/observability-online-eval/together/.venv/lib/python3.12/site-packages/together/abstract/api_requestor.py:635\u001b[39m, in \u001b[36mAPIRequestor._interpret_response\u001b[39m\u001b[34m(self, result, stream)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    633\u001b[39m     content = result.content.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    642\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Codebase/maxim-cookbooks/python/observability-online-eval/together/.venv/lib/python3.12/site-packages/together/abstract/api_requestor.py:707\u001b[39m, in \u001b[36mAPIRequestor._interpret_response_line\u001b[39m\u001b[34m(self, rbody, rcode, rheaders, stream)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m200\u001b[39m <= rcode < \u001b[32m300\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_error_response(resp, rcode, stream_error=stream)\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[31mInvalidRequestError\u001b[39m: Error code: 404 - {\"message\": \"Unable to access model meta-llama/Meta-Llama-3.2-8B. Please visit https://api.together.ai/models to view the list of supported models.\", \"type_\": \"invalid_request_error\", \"code\": \"model_not_available\"}"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What are some fun things to do in New York?\"}]\n",
    "trace.event(str(uuid4()), \"test event\")\n",
    "# Get and log response\n",
    "completion = get_together_response(messages)\n",
    "\n",
    "# Print result\n",
    "print(\"\\nTogether AI Response:\")\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
