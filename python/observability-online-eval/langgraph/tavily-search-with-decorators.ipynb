{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph agent example\n",
    " \n",
    "This notebook demonstrates how to use the Tavily search API with LangChain and LangGraph to create an agent that can search for information on the web. The agent uses either OpenAI or Anthropic models to process the search results and generate responses.\n",
    "\n",
    "We will also integrate Maxim AI tracing using LangGraph decorators.\n",
    " \n",
    "## Setup\n",
    "- Requires API keys for OpenAI or Anthropic and Tavily\n",
    "- Uses LangGraph for agent workflow orchestration\n",
    "- Implements ReAct pattern for reasoning and action\n",
    "- Integrates with Maxim for tracing and debugging agent execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agent diagram](./files/tavily-agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State and Tools\n",
    " \n",
    "- `AgentState`: TypedDict that tracks the conversation state with messages\n",
    "- Tools: Using Tavily Search API to retrieve information from the web\n",
    "- Model Selection: Function to choose between OpenAI and Anthropic models\n",
    "- Control Flow: Logic to determine when the agent should continue or finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "\n",
    "from langchain_anthropic import  ChatAnthropic\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph, add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openAIKey = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "anthropicApiKey = os.environ.get(\"ANTHROPIC_API_KEY\", None)\n",
    "tavilyApiKey = os.environ.get(\"TAVILY_API_KEY\", None)\n",
    "workspaceId = os.environ.get(\"MAXIM_WORKSPACE_ID\", None)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1, tavily_api_key=tavilyApiKey)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def _get_model(model_name: str):\n",
    "    if model_name == \"openai\":\n",
    "        model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", api_key=openAIKey)\n",
    "    elif model_name == \"anthropic\":\n",
    "        model = ChatAnthropic(\n",
    "            temperature=0,\n",
    "            model_name=\"claude-3-sonnet-20240229\",\n",
    "            api_key=anthropicApiKey,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    model = model.bind_tools(tools)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Be a helpful assistant\"\"\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state, config):\n",
    "    messages = state[\"messages\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    model_name = config.get(\"configurable\", {}).get(\"model_name\", \"anthropic\")\n",
    "    model = _get_model(model_name)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "\n",
    "# Define the config\n",
    "class GraphConfig(TypedDict):\n",
    "    model_name: Literal[\"anthropic\", \"openai\"]\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState, config_schema=GraphConfig)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxim Logger Initialization\n",
    "\n",
    "Documentation: https://www.getmaxim.ai/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[MaximSDK] Initializing Maxim AI(v3.9.2)\u001b[0m\n",
      "\u001b[32m[MaximSDK] Using info logging level.\u001b[0m\n",
      "\u001b[32m[MaximSDK] For debug logs, set global logging level to debug logging.basicConfig(level=logging.DEBUG).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from maxim import  Maxim\n",
    "from maxim.decorators import current_trace, span, trace\n",
    "from maxim.decorators.langchain import langchain_callback, langgraph_agent\n",
    "\n",
    "maxim_client = Maxim()\n",
    "logger = maxim_client.logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tracing entire LangGraph agent using 2 simle annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@span(name=\"another-method-span\")\n",
    "def another_method(query: str) -> str:\n",
    "    return query\n",
    "\n",
    "\n",
    "@langgraph_agent(name=\"movie-agent-v1\")\n",
    "async def ask_agent(query: str) -> str:\n",
    "    config = {\"recursion_limit\": 50, \"callbacks\": [langchain_callback()]}\n",
    "    async for event in app.astream(input={\"messages\": [query]}, config=config):\n",
    "        for k, v in event.items():\n",
    "            if k == \"agent\":\n",
    "                response = str(v[\"messages\"][0].content)\n",
    "    return response\n",
    "\n",
    "\n",
    "@trace(logger=logger, name=\"movie-chat-v1\",tags={\"service\": \"movie-chat-v1-server-1\"})\n",
    "async def handle(query: str):\n",
    "    resp = await ask_agent(query)\n",
    "    current_trace().set_output(str(resp))\n",
    "    another_method(str(resp))\n",
    "    trace = current_trace()\n",
    "    trace.feedback({\"score\": 1})\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, there does not appear to be a new Iron Man movie coming out in 2023. However, the Disney+ series Armor Wars is set to release in 2023 and will feature Robert Downey Jr. reprising his role as Tony Stark/Iron Man from the Marvel Cinematic Universe films.\n",
      "\n",
      "The key relevant information is:\n",
      "\n",
      "\"Armor Wars is set after the events of Secret Invasion (2023). Walton Goggins is set to reprise his role as Sonny Burch, with Robert Downey Jr. the first confirmed to reprise his role as Tony Stark / Iron Man.\"\n",
      "\n",
      "So while there is no new theatrical Iron Man movie planned for 2023, fans of the character can look forward to Downey Jr.'s return as Iron Man in the Armor Wars Disney+ series later this year. But there are no announced plans for a new standalone Iron Man film in 2023.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resp = await handle(\"is there any new iron man movies coming this year?\")    \n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running tests on this agent on Maxim using local dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim.models import LocalData, DataStructure, YieldedOutput\n",
    "\n",
    "dataset:list[LocalData] = [\n",
    "    {\"input\" : \"Who is the current president of the United States?\"},\n",
    "    {\"input\" : \"What is the capital of France?\"},\n",
    "    {\"input\":\"What is the capital of India?\"},\n",
    "    {\"input\":\"What is the capital of Japan?\"},\n",
    "]\n",
    "\n",
    "data_structure:DataStructure = {\n",
    "    \"input\":\"INPUT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggering test run using the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run(row: LocalData):\n",
    "    response = await handle(row.get(\"input\"))\n",
    "    return YieldedOutput(\n",
    "        data=response,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim_client.create_test_run(\n",
    "    name=\"Local dataset test run from SDK\", in_workspace_id=workspaceId\n",
    ").with_concurrency(2).with_data(dataset).with_data_structure(data_structure).yields_output(run).with_evaluators(\n",
    "    \"Bias\"\n",
    ").run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
