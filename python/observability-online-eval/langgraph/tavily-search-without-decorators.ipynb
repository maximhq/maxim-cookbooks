{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph agent example\n",
    " \n",
    "This notebook demonstrates how to use the Tavily search API with LangChain and LangGraph to create an agent that can search for information on the web. The agent uses either OpenAI or Anthropic models to process the search results and generate responses.\n",
    " \n",
    "## Setup\n",
    "- Requires API keys for OpenAI or Anthropic and Tavily\n",
    "- Uses LangGraph for agent workflow orchestration\n",
    "- Implements ReAct pattern for reasoning and action\n",
    "- Integrates with Maxim for tracing and debugging agent execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agent diagram](./files/tavily-agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State and Tools\n",
    " \n",
    "- `AgentState`: TypedDict that tracks the conversation state with messages\n",
    "- Tools: Using Tavily Search API to retrieve information from the web\n",
    "- Model Selection: Function to choose between OpenAI and Anthropic models\n",
    "- Control Flow: Logic to determine when the agent should continue or finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "\n",
    "from langchain_anthropic import  ChatAnthropic\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph, add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10d721af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openAIKey = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "anthropicApiKey = os.environ.get(\"ANTHROPIC_API_KEY\", None)\n",
    "tavilyApiKey = os.environ.get(\"TAVILY_API_KEY\", None)\n",
    "workspaceId = os.environ.get(\"MAXIM_WORKSPACE_ID\", None)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1, tavily_api_key=tavilyApiKey)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def _get_model(model_name: str):\n",
    "    if model_name == \"openai\":\n",
    "        model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", api_key=openAIKey)\n",
    "    elif model_name == \"anthropic\":\n",
    "        model = ChatAnthropic(\n",
    "            temperature=0,\n",
    "            model_name=\"claude-3-sonnet-20240229\",\n",
    "            api_key=anthropicApiKey,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    model = model.bind_tools(tools)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Be a helpful assistant\"\"\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state, config):\n",
    "    messages = state[\"messages\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    model_name = config.get(\"configurable\", {}).get(\"model_name\", \"anthropic\")\n",
    "    model = _get_model(model_name)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "\n",
    "# Define the config\n",
    "class GraphConfig(TypedDict):\n",
    "    model_name: Literal[\"anthropic\", \"openai\"]\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState, config_schema=GraphConfig)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxim Logger Initialization\n",
    "\n",
    "Documentation: https://www.getmaxim.ai/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[MaximSDK] Initializing Maxim AI(v3.9.2)\u001b[0m\n",
      "\u001b[32m[MaximSDK] Using info logging level.\u001b[0m\n",
      "\u001b[32m[MaximSDK] For debug logs, set global logging level to debug logging.basicConfig(level=logging.DEBUG).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from maxim import  Maxim\n",
    "from maxim.decorators import current_trace, span, trace\n",
    "from maxim.decorators.langchain import langchain_callback, langgraph_agent\n",
    "\n",
    "maxim_client = Maxim()\n",
    "logger = maxim_client.logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tracing entire LangGraph agent using 2 simle annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()\n",
    "\n",
    "@span(name=\"another-method-span\")\n",
    "def another_method(query:str)->str:\n",
    "    return query\n",
    "\n",
    "@langgraph_agent(name=\"movie-agent-v1\")\n",
    "async def ask_agent(query: str) -> str:\n",
    "    config = {\"recursion_limit\": 50, \"callbacks\": [langchain_callback()]}\n",
    "    async for event in app.astream(input={\"messages\": [query]}, config=config):\n",
    "        for k, v in event.items():\n",
    "            if k == \"agent\":\n",
    "                response = str(v[\"messages\"][0].content)\n",
    "    return response\n",
    "\n",
    "@trace(logger=logger, name=\"movie-chat-v1\", evaluators=[\"bias\"])\n",
    "async def handle(query:str):\n",
    "    resp = await ask_agent(query)\n",
    "    current_trace().set_output(str(resp))\n",
    "    another_method(str(resp))\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search results provide some of the latest news stories from the NFL's website, including:\n",
      "\n",
      "- Comments from the San Francisco 49ers GM John Lynch about their offseason moves to get younger and cheaper.\n",
      "- News of the Pittsburgh Steelers extending safety DeShon Elliott's contract.\n",
      "- Quotes from new Houston Texans WR Christian Kirk praising rookie QB C.J. Stroud.\n",
      "- San Francisco 49ers TE George Kittle saying tight ends deserve to get paid more.\n",
      "- Defensive coordinator Dan Quinn praising the speed of WR Deebo Samuel.\n",
      "\n",
      "The top stories cover things like major player transactions, contract extensions, quotes from players/coaches, and other breaking news from around the NFL. Let me know if you need any other details on the latest football happenings!\n"
     ]
    }
   ],
   "source": [
    "resp = await handle(\"tell me latest football news?\")    \n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running tests on this agent on Maxim using local dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim.models import LocalData, DataStructure, YieldedOutput\n",
    "\n",
    "dataset:list[LocalData] = [\n",
    "    {\"input\" : \"Who is the current president of the United States?\"},\n",
    "    {\"input\" : \"What is the capital of France?\"},\n",
    "    {\"input\":\"What is the capital of India?\"},\n",
    "    {\"input\":\"What is the capital of Japan?\"},\n",
    "]\n",
    "\n",
    "data_structure:DataStructure = {\n",
    "    \"input\":\"INPUT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggering test run using the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run(row: LocalData):\n",
    "    response = await handle(row.get(\"input\"))\n",
    "    return YieldedOutput(\n",
    "        data=response,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim_client.create_test_run(\n",
    "    name=\"Local dataset test run from SDK\", in_workspace_id=workspaceId\n",
    ").with_concurrency(2).with_data(dataset).with_data_structure(data_structure).yields_output(run).with_evaluators(\n",
    "    \"Bias\"\n",
    ").run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
