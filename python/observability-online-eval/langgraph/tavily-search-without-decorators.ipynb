{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tavily Search Integration with LangChain and LangGraph\n",
    " \n",
    "This notebook demonstrates how to use the Tavily search API with LangChain and LangGraph to create an agent that can search for information on the web. The agent uses either OpenAI or Anthropic models to process the search results and generate responses.\n",
    " \n",
    "## Setup\n",
    "- Requires API keys for OpenAI or Anthropic and Tavily\n",
    "- Uses LangGraph for agent workflow orchestration\n",
    "- Implements ReAct pattern for reasoning and action\n",
    "- Integrates with Maxim for tracing and debugging agent execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Annotated, List, Literal, Sequence, Tuple, TypedDict, Union\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_anthropic import Anthropic, ChatAnthropic\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph, add_messages\n",
    "from langgraph.prebuilt import ToolNode, create_react_agent\n",
    "from pydantic.type_adapter import R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State and Tools\n",
    " \n",
    "- `AgentState`: TypedDict that tracks the conversation state with messages\n",
    "- Tools: Using Tavily Search API to retrieve information from the web\n",
    "- Model Selection: Function to choose between OpenAI and Anthropic models\n",
    "- Control Flow: Logic to determine when the agent should continue or finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openAIKey = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "anthropicApiKey = os.environ.get(\"ANTHROPIC_API_KEY\", None)\n",
    "tavilyApiKey = os.environ.get(\"TAVILY_API_KEY\", None)\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1, tavily_api_key=tavilyApiKey)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def _get_model(model_name: str):\n",
    "    if model_name == \"openai\":\n",
    "        model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", api_key=openAIKey)\n",
    "    elif model_name == \"anthropic\":\n",
    "        model = ChatAnthropic(\n",
    "            temperature=0,\n",
    "            model_name=\"claude-3-sonnet-20240229\",\n",
    "            api_key=anthropicApiKey,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    model = model.bind_tools(tools)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Be a helpful assistant\"\"\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state, config):\n",
    "    messages = state[\"messages\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    model_name = config.get(\"configurable\", {}).get(\"model_name\", \"anthropic\")\n",
    "    model = _get_model(model_name)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "\n",
    "# Define the config\n",
    "class GraphConfig(TypedDict):\n",
    "    model_name: Literal[\"anthropic\", \"openai\"]\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState, config_schema=GraphConfig)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxim Logger Initialization\n",
    "\n",
    "Documentation: https://www.getmaxim.ai/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim import  Maxim\n",
    "from maxim.decorators import current_trace\n",
    "\n",
    "logger = Maxim({}).logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tracing entire LangGraph agent using 2 simle annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim.logger.langchain import MaximLangchainTracer\n",
    "\n",
    "app = workflow.compile()\n",
    "maxim_langchain_tracer = MaximLangchainTracer(logger)\n",
    "\n",
    "\n",
    "def another_method(query:str)->str:\n",
    "    return query\n",
    "\n",
    "async def ask_agent(query: str) -> str:\n",
    "    config = {\"recursion_limit\": 50, \"callbacks\": [maxim_langchain_tracer]}\n",
    "    async for event in app.astream(input={\"messages\": [query]}, config=config):\n",
    "        for k, v in event.items():\n",
    "            if k == \"agent\":\n",
    "                response = str(v[\"messages\"][0].content)\n",
    "    return response\n",
    "\n",
    "async def handle(query:str):\n",
    "    resp = await ask_agent(query)    \n",
    "    another_method(str(resp))\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = await handle(\"tell me latest football news?\")    \n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
