{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84bff7d7-1ecd-4299-8df8-1e212cd0ec3d",
   "metadata": {},
   "source": [
    "# Building a RAG application and evaluating its performance using LLM-as-a-judge and Maxim AI\n",
    "\n",
    "In this cookbook, we'll create a RAG application using the following tools:\n",
    "- **MongoDB**: We'll use MongoDB atlas as a vector database for retrieval of relevant information from our knowledge base. \n",
    "- **OpenAI**: We'll use openAI's SOTA models to:\n",
    "    - Create embeddings\n",
    "    - Generate response from retrieved-context\n",
    "    - creating an LLM-as-a-judge evaluator to evaluate the relevance of our context\n",
    "- **Maxim AI**: We'll use Maxim's suite to:\n",
    "    - Trace the workflows of our RAG application (input, retrieval and generation)\n",
    "    - Evaluate the quality of input, context and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23bd2b5-05e8-48fc-8ded-99879f026377",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f014023-5a86-42a5-9e18-023830c8f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "import json\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc61dbf-aa58-4de9-b8a6-017a8156c3f0",
   "metadata": {},
   "source": [
    "- Follow this [guide to fetch you connection string (URI) from mongoDB.](https://www.mongodb.com/docs/guides/atlas/connection-string/) \n",
    "- Follow this [guide to generate and export your OpenAI API key.](https://platform.openai.com/docs/quickstart#create-and-export-an-api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e539259f-a964-426f-9f0f-e106f08c5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = os.getenv(\"MONGO_URI\") \n",
    "openAI_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=openAI_key)\n",
    "openai.api_key = openAI_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841b884-5161-4fd8-addf-c649fa9198c4",
   "metadata": {},
   "source": [
    "### Reading File and Creating chunks\n",
    "We'll start with reading our .txt file and chunking the text. \n",
    "\n",
    "**Chunking**: breaking large texts into smaller and manageable semantic units. Key benefits of chunking:\n",
    "- <u>Handle embedding API token limits</u>- if the text is too long, API will reject the request\n",
    "- <u>Be specific</u>- Smaller chunks capture local meaning better + stays relevant to context\n",
    "- <u>Memory Management</u>- large texts can overwhelm system memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb77b206-6812-438b-a355-d4f50912e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a txt file and return its entire content as a single string\n",
    "def read_txt_file(filePath):\n",
    "    with open(filePath, \"r\",encoding='utf-8') as file:\n",
    "        fileText = file.read() # return single string of all the text\n",
    "    return fileText\n",
    "\n",
    "# function to split the string into smaller chunks of specified size.\n",
    "def chunk_txt(text, chunkSize): # based on number of chars\n",
    "    chunkedText = [text[i:i+chunkSize] for i in range(0, len(text), chunkSize)]\n",
    "    return chunkedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416ef5c-5b33-4a85-a40a-6535ddbe5898",
   "metadata": {},
   "source": [
    "Our RAG application will be based on the Harry Potter and the Deathly Hallows book. <.txt file to be uploaded to maximHQ Github and linked here> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d3c1d9-b92d-4c58-9ab2-78806f1a1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment following lines of code to read file and split the text into chunks\n",
    "# textData= read_txt_file(\"harry-potter-deathly-hallows.txt\")\n",
    "# chunks = chunk_txt(textData, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c50730-2505-461b-9a0f-716142bbad13",
   "metadata": {},
   "source": [
    "### Creating Vector Embeddings\n",
    "**Embeddings** are vector representations of text that capture semantic meaning. We'll use openAI's [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings#embedding-models) embedding model.\n",
    "\n",
    "We'll create embeddings for a list of text chunks i.e., the entire content of our book split into smaller parts. This enables us to vectorize the knowledge base for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40ff251-48a6-4a4a-ac07-94b942bdc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function: generates embeddings for input text using OpenAI's embedding model\n",
    "def get_embedding(text):\n",
    "    response = openai.embeddings.create(\n",
    "            model= \"text-embedding-ada-002\",\n",
    "            input= [text] )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# creates embeddings of each chunk and return a list of documents containing text and corresponding embedding.\n",
    "def embed_chunks(chunks):\n",
    "    documents = []\n",
    "    for i in range(0, len(chunks)):\n",
    "        document = {\n",
    "            \"text\": chunks[i],\n",
    "            \"embedding\": get_embedding(chunks[i]),\n",
    "        }\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5205183-afb4-492b-af28-4e47e1a074db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = MongoClient(mongo_uri)\n",
    "db = mongo_client[\"sample_mflix\"] # replace \"sample_mflix\" with the name of your database\n",
    "collection = db[\"hp_embedding\"] # replace \"hp_embedding\" with the name of your collection\n",
    "\n",
    "\n",
    "# uncomment the following lines of code to store embeddings in you collection \n",
    "\n",
    "# documents = embed_chunks(chunks)\n",
    "# collection.insert_many(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae94903-7881-4e76-86c9-a68e25916157",
   "metadata": {},
   "source": [
    "### Creating Vector Search Index\n",
    "Next, we need to create a vector search index in MongoDB for semantic similarity search. Vector index enables efficient retrieval of relevant context based on similarity with user query.\n",
    "\n",
    "Here, we'll define **Cosine similarity** for efficient retrieval of relevant context based on query similarity. Cosine similarity measures cosine of the angle between two vectors i.e., 1-meaning the vectors are identical and 0-meaning they have no correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73f4b56-5701-473d-ae91-e23c26d4d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector index for our collection in MongoDB\n",
    "def create_vector_index(collection, indexName, fieldName):\n",
    "\n",
    "    search_index_model = SearchIndexModel(\n",
    "      definition={\n",
    "        \"fields\": [\n",
    "          {\n",
    "            \"type\": \"vector\",\n",
    "            \"numDimensions\": 1536,\n",
    "            \"path\": fieldName,#name of the field where embeddings are stored, here \"embedding\"\n",
    "            \"similarity\":  \"cosine\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      name= indexName,\n",
    "      type=\"vectorSearch\",\n",
    "    )\n",
    "    collection.create_search_index(model=search_index_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451da5e7-2c34-4302-877b-5508b812bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line of code to create a vector index for your collection\n",
    "\n",
    "# create_vector_index(collection,\"vector_index_hp\", \"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b4b44-288f-4e2d-98b9-a2817fb7dbd8",
   "metadata": {},
   "source": [
    "### Retrieving context from Vector DB\n",
    "Creating a retriever function that calls mongoDBs to fetch relevant context from the vector database based on input query. The vector database uses semantic similarity to find the \"k\" most relevant text chunks to our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a129c0-76fd-4246-bdb7-9606efbf071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to  retrieve context from vector database using semantic search.\n",
    "def retrieve_context(query: str):\n",
    "    query_vector = get_embedding(query)\n",
    "    index_name = \"vector_index_hp\"\n",
    "    field_name = \"embedding\"\n",
    "    \n",
    "    response = collection.aggregate([\n",
    "        {\n",
    "            '$vectorSearch': {\n",
    "                \"index\": index_name, #name of the vector index\n",
    "                \"path\": field_name, #name of the field where embeddings are stored\n",
    "                \"queryVector\": query_vector,\n",
    "                \"numCandidates\": 50,\n",
    "                \"limit\": 10 #top k chunks to be retrieved\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                'text' : 1,\n",
    "                \"search_score\": { \"$meta\": \"vectorSearchScore\" }\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "    context = [item.get('text', 'N/A') for item in response]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ba4cd-adb6-4f6e-9d37-35d76f34f872",
   "metadata": {},
   "source": [
    "### Generating LLM Response\n",
    "\n",
    "We'll use openAIs `gpt-4o` model for generation of the response. We'll pass the user query and retrieved context to the LLM to generate a response grounded in the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd88c193-5330-4813-bda4-e7411fa2bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str):\n",
    "    context = retrieve_context(query)\n",
    "    prompt = (\n",
    "        f\"You are a smart agent. A question will be asked to you along with relevant context.\"\n",
    "        f\"Your task is to answer the question using the information provided.\"\n",
    "        f\"Question: {query}. Context: {context}\"\n",
    "    )\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407289f6-86ae-41c4-bd4e-32e446d2213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the Deathly Hallows?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c6e3b4-5579-4855-bab0-4f0f850c7749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n“Sorry?” said Hermione, sounding confused.\\n“Those are the Deathly Hallows,” said Xenophilius.\\nHe picked up a quill from a packed table at his elbow, and pulled a torn piece of parchment from between more books.\\n“The Elder Wand,” he said, and he drew', 'eyebrows.\\n“Are you referring to the sign of the Deathly Hallows?”\\n\\n\\x0cChapter 21\\nThe Tale of The Three Brothers\\n\\nHarry turned to look at Ron and Hermione. Neither of them seemed to have understood what Xenophilius had said either.\\n“The Deathly Hallows?', 'd come true.\\n“And at the heart of our schemes, the Deathly Hallows! How they fascinated him, how they fascinated both of us! The unbeatable wand, the weapon that would lead us to power! The Resurrection Stone — to him, though I pretended not to know ', '“What are you talking about?” asked Harry, startled by Dumbledore’s tone, by the sudden tears in his eyes.\\n“The Hallows, the Hallows,” murmured Dumbledore. “A desperate man’s dream!”\\n“But they’re real!”\\n“Real, and dangerous, and a lure for fools,” sa', 'more likely that the Peverell brothers were simply gifted, dangerous wizards who succeeded in creating those powerful objects. The story of them being Death’s own Hallows seems to me the sort of legend that might have sprung up around such creations.', 'rmione. “Together,” he said, “the Deathly Hallows.”\\n“But there’s no mention of the words ‘Deathly Hallows’ in the story,” said Hermione.\\n“Well, of course not,” said Xenophilius, maddeningly smug. “That is a children’s tale, told to amuse rather than ', 'owerful wands for hundreds of years.”\\n“There have?” asked Harry.\\nHermione looked exasperated: The expression was so endearingly familiar that Harry and Ron grinned at each other.\\n“The Deathstick, the Wand of Destiny, they crop up under different name', 'I had learned nothing. I was unworthy to unite the Deathly Hallows, I had proved it time and again, and here was final proof.”\\n“Why?” said Harry. “It was natural! You wanted to see them again. What’s wrong with that?”\\n“Maybe a man in a million could ', 'to instruct. Those of us who understand these matters, however, recognize that the ancient story refers to three objects, or Hallows, which, if united, will make the possessor master of Death.”\\nThere was a short silence in which Xenophilius glanced o', 'n of the Deathly Hallows on Ignotus’s grave is conclusive proof!”\\n“Of what?” asked Ron.\\n“Why, that the three brothers in the story were actually the three Peverell brothers, Antioch, Cadmus, and Ignotus! That they were the original owners of the Hall']\n"
     ]
    }
   ],
   "source": [
    "context = retrieve_context(query)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01773eb1-d017-44dd-92dc-ed222a243dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Deathly Hallows are three magical objects that are said to grant their possessor mastery over death. These objects are:\n",
      "\n",
      "1. **The Elder Wand** - An unbeatable wand with unparalleled power.\n",
      "2. **The Resurrection Stone** - A stone that can bring back the dead, albeit not truly resurrecting them.\n",
      "3. **The Invisibility Cloak** - A cloak that renders the wearer completely invisible.\n",
      "\n",
      "In the context of the story, they are tied to the Peverell brothers, who were said to have each possessed one of these objects. The legend around them suggests that uniting all three Hallows would make someone the Master of Death.\n"
     ]
    }
   ],
   "source": [
    "llm_response = generate_response(query)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36138f6-ceef-42cf-9d8e-71118e544620",
   "metadata": {},
   "source": [
    "## Creating LLM-as-a-judge Evaluator\n",
    "Now we'll create an LLM-as-a-judge evaluator to evaluate the context relevance of our workflow. We'll use OpenAI's `gpt-4o-mini` model to evaluate the responses.\n",
    "\n",
    "<u>Context relevance</u>: assesses the effectiveness of your RAG pipeline's retriever by determining how relevant the information in the retrieved context is to the given input.\n",
    "  in simple terms, Context Relevance = Number of Relevant Statements / Total Number of Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fee518e-cb20-4e3b-80b3-417c83467624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LLM as a Judge to evaluate Context relevance\n",
    "def context_relevance(input, context, prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt.format(input=input, context=context),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479af48-a01d-4f18-8ddf-08194553fab4",
   "metadata": {},
   "source": [
    "### #1- Basic scoring\n",
    "For the first version of our Context Relevance Evaluator, we will directly prompt the model to rate the relevance of the given context on a scale from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b382ab6-64d5-4cd3-8c5a-e51d94492b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "You are a CONTEXTUAL RELEVANCE EVALUATOR who has to score how relevant the context given is, based on the given input:\n",
    "Rate the submission on a scale of 1 to 5.\n",
    "Input: {input}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "response_1 = context_relevance(query, context, prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f802b90f-151b-4379-bc8d-a94009cbb0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would rate the relevance of the context provided to the input \"What are the Deathly Hallows?\" as a 5. \\n\\nThe context consists of multiple excerpts discussing the Deathly Hallows directly, which clearly addresses the subject of the input. It includes quotes from characters and details about the Hallows themselves, such as their significance, the legend surrounding them, and their connection to other characters in the narrative. This makes the context highly relevant and informative regarding the query about what the Deathly Hallows are.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94dd27-57f9-44e0-be15-4d52c4e1534c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ebea31-294f-4a02-a336-303174f64fb9",
   "metadata": {},
   "source": [
    "### #2 Added reasoning\n",
    "We'll further refine our prompt and instruct the model to assign scores based on a set of criteria. We'll also prompt the model to give the reason behind the score given to get better insights into the logic applied behind the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09098a44-0a6f-4169-8242-543d93fc8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Score the context's relevancy to the input from 1-5 based on:\n",
    "1. Topic match: Keywords and subject alignment of input and context\n",
    "2. Quality of the context: Required information which are needed for input are present in the context\n",
    "Also, provide a reason for the score given.\n",
    "Example:\n",
    "Example Context: \"AlphaFold won the Nobel Prize in 2025. In the same year, OpenAI released the O3 model with reasoning. There was a cat on the road.\"\n",
    "Example Input: \"What were the biggest events of 2024?\"\n",
    "Example Score: 3.3\n",
    "Example Reason: Two out of the three sentences in the context are relevant to the input\n",
    "Input: {input}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "response_2 = context_relevance(query, context, prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be883066-664c-46d8-8f27-db5d4b2d3389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Score: 4.7\\n\\nReason: The context is highly relevant to the input as it directly discusses the Deathly Hallows. There are multiple mentions of the Deathly Hallows in different contexts within the provided text, ensuring a strong alignment with the topic. Additionally, the context includes explanations and references that inform the reader about the nature of the Deathly Hallows, their significance, and their connection to characters in the narrative. The only reason it's not a perfect score is that the context may require some prior knowledge of the story for complete clarity, but overall, it provides substantial information.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831741d-5b01-485e-810f-e7aa2b6b03a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90e44a12-25bf-4112-b155-ec69d92a2aa8",
   "metadata": {},
   "source": [
    "### #3 Comprehensive evaluation with thought process\n",
    "In this version, the model will be prompted to engage in a chain of thought process to analyze the broader scope of the situation. For scoring, we will define criteria that encourage the model to apply critical thinking and structured reasoning.\n",
    "\n",
    "We'll additionally use OpenAI's structured response feature to convert the output of our evaluator into callable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a10c7791-4e65-45b3-a9a4-848a5d2cbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2176612f-d51a-4354-91d6-9774edec46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning the thought and score of LLM-as-a-judge evaluator in structered way\n",
    "class Reasoning(BaseModel):\n",
    "    thought: str\n",
    "    score: str\n",
    "client = OpenAI()\n",
    "\n",
    "prompt_3 = \"\"\"\n",
    "You are an evaluator who analyzes if the context is relevant to the input.\n",
    "Before scoring, analyze in the thought:\n",
    "1. What does the input ask for?\n",
    "2. What information does the context provide?\n",
    "3. What's missing or irrelevant?\n",
    "Then score (1-5):\n",
    "1. Topic match: Keywords and subject alignment of input and context\n",
    "2. Quality of the context: Required information which are needed for input are present in the context\n",
    "Explain your reasoning and give the total score.\n",
    "Example:\n",
    "Example Input: \"What is photosynthesis?\"\n",
    "Example Context: \"Photosynthesis is how plants make food using sunlight, water, and carbon dioxide. We have many trees in Bangalore.\"\n",
    "Example Thought: \"The input asks for photosynthesis. The first statement in the context addresses it while not the second.\"\n",
    "Example Score: 2.5\n",
    "Example Reason: Out of the two statements, only one context statement is relevant to the input hence a score of 0.5\n",
    "Input: {input}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "response_3 = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_3.format(input=query, context=context),\n",
    "        }\n",
    "    ],\n",
    "    response_format=Reasoning,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3a8ddbc-1e2f-4647-b2de-510b8cba2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_output= response_3.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8989056d-5647-46a3-b72d-e5150d31ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator score: 3.5\n",
      "Reason for the score: 1. The input asks for information about 'the Deathly Hallows.' Specifically, it seems to inquire about their definition or significance in the context of the Harry Potter universe.  \n",
      "2. The context provides dialogue from a scene where characters discuss the Deathly Hallows and their history, including mentions of the Elder Wand, Resurrection Stone, and the tale of the three brothers, which are key elements related to the Deathly Hallows. However, it lacks a clear, concise definition or explanation of what the Deathly Hallows are, instead relying on indirect references and dialogue.  \n",
      "3. The context includes various characters discussing the Deathly Hallows but does not explicitly define them or explain their importance comprehensively. Therefore, while there is relevant material, the clarity and completeness needed to accurately answer the input are missing.\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluator score:\", structured_output.score)\n",
    "print(\"Reason for the score:\", structured_output.thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128a65e-785c-4fc8-ba8c-e54d92a4b498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac728fb7-d6ad-4b34-bef8-4bb27a2de1fe",
   "metadata": {},
   "source": [
    "## Using Maxim AI\n",
    "Maxim is an end-to-end [AI evaluation and observability](https://www.getmaxim.ai/) platform. \n",
    "\n",
    "### Getting started\n",
    "We'll use Maxim's SDK to trace and evaluate our RAG workflow.\n",
    "- Get started for free by [signing up here](https://app.getmaxim.ai/sign-up).\n",
    "- Follow this [guide to generate the Maxim API key](https://www.getmaxim.ai/docs/sdk/observability/python/overview#generate-maxim-api-key) and ensure to copy the API key before closing the dialog.\n",
    "\n",
    "Ref: [Installation steps to get started with Maxim's python SDK](https://www.getmaxim.ai/docs/sdk/observability/python/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bce2f65-3cd8-41b2-99f1-2b44c104cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim import Config, Maxim\n",
    "from maxim.logger import Logger, LoggerConfig\n",
    "\n",
    "from maxim.logger import TraceConfig, SpanConfig, GenerationConfig, RetrievalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31083938-7d03-437a-9ec5-99787f734d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim_api_key = os.getenv(\"MAXIM_API_KEY\")\n",
    "maxim_log_key = os.getenv(\"LOG_REPOSITORY_ID\")\n",
    "\n",
    "maxim = Maxim(Config(api_key=maxim_api_key))\n",
    "logger = maxim.logger(LoggerConfig(id=maxim_log_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffcbc2-ad94-4bf4-9468-90aee1f13a57",
   "metadata": {},
   "source": [
    "### Tracing and evaluating RAG components using Maxim SDK\n",
    "We'll trace the generation and retrieval using Maxim's SDK. Maxim enables us to attach LLM-as-a-judge evaluators directly to our traces for continuous evaluation. \n",
    "\n",
    "Components of Maxim's [logging heirarchy](https://www.getmaxim.ai/docs/observability/concepts#components-of-logs):\n",
    "- [Session](https://www.getmaxim.ai/docs/observability/concepts#session): it is the top level entity that captures all the multi-turn interactions of your system. \n",
    "- [Trace](https://www.getmaxim.ai/docs/observability/concepts#trace): a trace is the complete processing of a request through a distributed system, including all the actions between the request and the response.\n",
    "- [Span](https://www.getmaxim.ai/docs/observability/concepts#span): Spans are fundamental building blocks of distributed tracing. A single trace in distributed tracing consists of a series of tagged time intervals known as spans\n",
    "- [Generation](https://www.getmaxim.ai/docs/observability/concepts#generation): A Generation represents a single Large Language Model (LLM) call within a trace or span. Multiple generations can exist within a single trace/span.\n",
    "- [Retrieval](https://www.getmaxim.ai/docs/observability/concepts#retrieval): A Retrieval (commonly used in RAG) represents a query operation to fetch relevant context or information from a knowledge base or vector database within a trace or span. \n",
    "\n",
    "Ref: [Tracing your workflow using Maxim](https://www.getmaxim.ai/docs/sdk/observability/python/manual-integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46faa15d-761e-4a0c-ab2b-787cf0c75ba8",
   "metadata": {},
   "source": [
    "#### Tracing our retriever function\n",
    "We'll log the input we're passing to our vector index and the context we're fetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8f649fa-935d-4f87-95ce-d784c3427b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_with_trace(query: str, span):\n",
    "    query_vector = get_embedding(query)\n",
    "    index_name = \"vector_index_hp\"\n",
    "    field_name = \"embedding\"\n",
    "    retrieval = span.retrieval(RetrievalConfig(id=str(uuid4())))\n",
    "    retrieval.input(query) #logging input to retriever i.e user query\n",
    "    \n",
    "    response = collection.aggregate([\n",
    "        {\n",
    "            '$vectorSearch': {\n",
    "                \"index\": index_name, #name of the vector index\n",
    "                \"path\": field_name, #name of the field where embeddings are stored\n",
    "                \"queryVector\": query_vector,\n",
    "                \"numCandidates\": 50,\n",
    "                \"limit\": 10 #top k chunks to be retrieved\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                'text' : 1,\n",
    "                \"search_score\": { \"$meta\": \"vectorSearchScore\" }\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "    context = [item.get('text', 'N/A') for item in response]\n",
    "    \n",
    "    retrieval.output(context) #logging the output of retrieval action i.e the context\n",
    "\n",
    "    retrieval.end()\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1eee02-8f9d-4e96-aec8-9f7875c02c2a",
   "metadata": {},
   "source": [
    "#### Tracing our LLM generation function and adding evaluators to logs using SDK\n",
    "We'll log the generated response and model parameters such as cost, tokens, and latency for performance monitoring.\n",
    "\n",
    "Further, using Maxim, we can attach evaluators to each level of our logging hierarchy (i.e., trace, span, or component within the span). Here, we'll:\n",
    "- <u>Evaluate trace</u>: to check the **relevance of our retrieved context** with respect to input and output. \n",
    "- <u>Evaluate llm generation</u>: to check the **clarity** of our models response and detect any **bias** in it.\n",
    "\n",
    "Read more about [adding evals at node level using Maxim](https://www.getmaxim.ai/docs/observability/evaluating-logs/agentic-evaluation#conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21ceeb35-87ea-400e-aab5-5f6a0b0ab4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_trace(query: str):\n",
    "    trace = logger.trace(TraceConfig(id=str(uuid4().hex)))\n",
    "    span = trace.span(SpanConfig(id=str(uuid4())))\n",
    "    \n",
    "    context = retrieve_context_with_trace(query, span)\n",
    "    prompt = (\n",
    "        f\"You are a smart agent. A question will be asked to you along with relevant context. \"\n",
    "        f\"Your task is to answer the question using the information provided. \"\n",
    "        f\"Question: {query}. Context: {context}\"\n",
    "    )\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # You can change this to \"gpt-4\" if you have access\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generationConfig = GenerationConfig(\n",
    "\t\t\t\t\tid=str(uuid4()),\n",
    "\t\t\t\t\tname=\"llmGeneration\",\n",
    "\t\t\t\t\tprovider=\"openAI\",\n",
    "\t\t\t\t\tmodel=\"gpt-4o\",\n",
    "\t\t\t\t\tmessages=[{\n",
    "\t\t\t\t\t\t\"role\": \"user\", \n",
    "\t\t\t\t\t\t\"content\": query # log the input to generation step\n",
    "\t\t\t\t\t}])\n",
    "    \n",
    "    generation = trace.generation(generationConfig) \n",
    "    generation.result(response) # Maxim's generation.result() expects result to be in OpenAI response format.\n",
    "\n",
    "    llm_response = response.choices[0].message.content\n",
    "\n",
    "# evaluating bias and clarity in our LLM's generated response\n",
    "    generation.evaluate().with_evaluators(\"clarity\", \"bias\").with_variables({\n",
    "            \"output\": llm_response\n",
    "        })\n",
    "    \n",
    "    trace.evaluate().with_evaluators(\"context relevance\")\n",
    "    trace.evaluate().with_variables(\n",
    "        { \n",
    "            \"output\": llm_response,\n",
    "            \"input\": query,\n",
    "            \"context\": context\n",
    "        }, \n",
    "        [\"context relevance\"] # List of evaluators\n",
    "    )\n",
    "    span.end()\n",
    "\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0ebcaee-abbb-49f1-816a-3cad73c85551",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"What is the Elder Wand?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbb4db41-6161-471c-918d-5796b456cfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Elder Wand is one of the most powerful wands in the wizarding world, often described as more powerful than any other wand. It is one of the Deathly Hallows, and is also known as the Deathstick or the Wand of Destiny. According to legend, it was created by Death himself and must always win duels for its true owner. The wand is unique in that its allegiance can change if its owner is disarmed or defeated, and it is said that the wand chooses the wizard. The wand's power is most effectively harnessed by its rightful master, the person who has won its allegiance.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response_with_trace(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3d8eb-afcc-459a-b09e-4591489671e0",
   "metadata": {},
   "source": [
    "![Maxim platform](rag-tracing-and-evaluation-openai.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727fa31d-33db-4da7-b21b-4f7f910324ee",
   "metadata": {},
   "source": [
    "## Use local dataset to trigger test runs on Maxim\n",
    "\n",
    "We'll programmatically trigger test runs using Maxim's SDK with custom datasets, flexible output functions, and evaluations for our RAG applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa2fda-7a41-450b-a352-ab50e0eb7d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
