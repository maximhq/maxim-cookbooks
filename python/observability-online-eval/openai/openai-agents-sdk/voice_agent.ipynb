{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "948cafc1",
   "metadata": {},
   "source": [
    "# Voice Agent using OpenAI Agents SDK with Maxim Logging\n",
    "\n",
    "This notebook demonstrates how to build a voice assistant using OpenAI's Agents SDK with Maxim tracing for observability.\n",
    "\n",
    "The assistant includes:\n",
    "- **Triage Agent**: Routes queries to appropriate specialized agents\n",
    "- **Search Agent**: Performs web search for real-time information\n",
    "- **Account Agent**: Provides account information via function calling\n",
    "- **Voice Pipeline**: Converts text-based agents to voice-based interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569901e1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from agents import Agent, Runner, add_trace_processor, function_tool, set_default_openai_key\n",
    "from agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n",
    "from agents.voice import AudioInput, SingleAgentVoiceWorkflow, TTSModelSettings, VoicePipeline, VoicePipelineConfig\n",
    "\n",
    "# Optional: Import WebSearchTool if available\n",
    "try:\n",
    "    from agents import WebSearchTool\n",
    "    WEB_SEARCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WEB_SEARCH_AVAILABLE = False\n",
    "    WebSearchTool = None\n",
    "    print(\"‚ö†Ô∏è WebSearchTool not available\")\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MAXIM_API_KEY = os.getenv(\"MAXIM_API_KEY\")\n",
    "MAXIM_LOG_REPO_ID = os.getenv(\"MAXIM_LOG_REPO_ID\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
    "\n",
    "# Set OpenAI API key for agents\n",
    "set_default_openai_key(OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85eeb4",
   "metadata": {},
   "source": [
    "### Initializing Maxim SDK\n",
    "\n",
    "- Maxim SDK automatically picks up `MAXIM_API_KEY` and `MAXIM_LOG_REPO_ID` from environment variables\n",
    "- The `MaximOpenAIAgentsTracingProcessor` automatically traces all agent interactions\n",
    "- Learn more [here](https://www.getmaxim.ai/docs/observe/concepts#log-repository)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maxim import Maxim, Config\n",
    "from maxim.logger.openai.agents import MaximOpenAIAgentsTracingProcessor\n",
    "\n",
    "# Creating a new logger instance\n",
    "# It automatically initializes using MAXIM_API_KEY and MAXIM_LOG_REPO_ID from env variables\n",
    "logger = Maxim(Config()).logger()\n",
    "\n",
    "# Add Maxim trace processor to automatically trace all agent interactions\n",
    "add_trace_processor(MaximOpenAIAgentsTracingProcessor(logger))\n",
    "print(\"‚úÖ Maxim logging enabled for Agents SDK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599e460",
   "metadata": {},
   "source": [
    "## Defining Tools\n",
    "\n",
    "We'll create a custom tool for account information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c422f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def get_account_info(user_id: str) -> dict:\n",
    "    \"\"\"Return account info for a given user ID.\"\"\"\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"name\": \"Bugs Bunny\",\n",
    "        \"account_balance\": \"¬£72.50\",\n",
    "        \"membership_status\": \"Gold Executive\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd3cfb",
   "metadata": {},
   "source": [
    "## Defining Agents\n",
    "\n",
    "We'll create specialized agents for different use cases, optimized for voice output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice-optimized system prompt\n",
    "VOICE_SYSTEM_PROMPT = \"\"\"\n",
    "[Output Structure]\n",
    "\n",
    "Your output will be delivered in an audio voice response, please ensure that every response meets these guidelines:\n",
    "\n",
    "1. Use a friendly, human tone that will sound natural when spoken aloud.\n",
    "2. Keep responses short and segmented‚Äîideally one to two concise sentences per step.\n",
    "3. Avoid technical jargon; use plain language so that instructions are easy to understand.\n",
    "4. Provide only essential details so as not to overwhelm the listener.\n",
    "\"\"\"\n",
    "\n",
    "# Search Agent\n",
    "search_agent = Agent(\n",
    "    name=\"SearchAgent\",\n",
    "    instructions=VOICE_SYSTEM_PROMPT + (\n",
    "        \"You immediately provide an input to the WebSearchTool to find up-to-date information on the user's query.\"\n",
    "    ),\n",
    "    tools=[WebSearchTool()] if WEB_SEARCH_AVAILABLE else [],\n",
    ")\n",
    "\n",
    "# Account Agent\n",
    "account_agent = Agent(\n",
    "    name=\"AccountAgent\",\n",
    "    instructions=VOICE_SYSTEM_PROMPT + (\n",
    "        \"You provide account information based on a user ID using the get_account_info tool.\"\n",
    "    ),\n",
    "    tools=[get_account_info],\n",
    ")\n",
    "\n",
    "# Triage Agent\n",
    "triage_agent = Agent(\n",
    "    name=\"VoiceAssistant\",\n",
    "    instructions=prompt_with_handoff_instructions(\"\"\"\n",
    "You are the virtual assistant for Acme Shop. Welcome the user and ask how you can help.\n",
    "\n",
    "Based on the user's intent, route to:\n",
    "- AccountAgent for account-related queries\n",
    "- SearchAgent for anything requiring real-time web search\n",
    "\"\"\"),\n",
    "    handoffs=[account_agent, search_agent],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48817aa3",
   "metadata": {},
   "source": [
    "## Voice Pipeline Configuration\n",
    "\n",
    "Configure the text-to-speech settings for natural, friendly voice output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf297443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom TTS model settings for natural voice output\n",
    "custom_tts_settings = TTSModelSettings(\n",
    "    instructions=(\n",
    "        \"Personality: upbeat, friendly, persuasive guide. \"\n",
    "        \"Tone: Friendly, clear, and reassuring, creating a calm atmosphere and making the listener feel confident and comfortable. \"\n",
    "        \"Pronunciation: Clear, articulate, and steady, ensuring each instruction is easily understood while maintaining a natural, conversational flow. \"\n",
    "        \"Tempo: Speak at a moderate, natural pace - not too fast, not too slow. Use strategic pauses between sentences and after important points. Include brief pauses before and after questions to allow the listener to process the information. \"\n",
    "        \"Emotion: Warm and supportive, conveying empathy and care, ensuring the listener feels guided and safe throughout the journey.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "voice_pipeline_config = VoicePipelineConfig(tts_settings=custom_tts_settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6773ea",
   "metadata": {},
   "source": [
    "## Voice Assistant Function\n",
    "\n",
    "This function handles the voice interaction loop, recording audio input and playing audio responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def voice_assistant_with_maxim():\n",
    "    \"\"\"Run voice assistant with Maxim tracing (automatic via trace processor).\"\"\"\n",
    "    # Use a standard sample rate for better quality and compatibility\n",
    "    # 16000 Hz is commonly used for speech recognition and provides good quality\n",
    "    SAMPLE_RATE = 16000  # Standard sample rate for speech\n",
    "    \n",
    "    print(\"üé§ Voice Assistant Ready!\")\n",
    "    print(\"Press Enter to speak your query (or type 'exit' to quit)\")\n",
    "    print(f\"üìä Audio settings: {SAMPLE_RATE} Hz, mono, 16-bit\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        # Check for input to either provide voice or exit\n",
    "        cmd = input(\"\\nPress Enter to speak (or type 'exit' to quit): \")\n",
    "        \n",
    "        if cmd.lower() == \"exit\":\n",
    "            print(\"üëã Exiting...\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            print(\"üé§ Listening...\")\n",
    "            recorded_chunks = []\n",
    "            \n",
    "            # Start streaming from microphone with fixed sample rate for better quality\n",
    "            with sd.InputStream(\n",
    "                samplerate=SAMPLE_RATE,\n",
    "                channels=1,\n",
    "                dtype='int16',\n",
    "                blocksize=4096,  # Larger block size for better quality\n",
    "                callback=lambda indata, frames, time, status: recorded_chunks.append(indata.copy())\n",
    "            ):\n",
    "                input()  # Wait for Enter key\n",
    "            \n",
    "            # Concatenate chunks into single buffer\n",
    "            recording = np.concatenate(recorded_chunks, axis=0)\n",
    "            \n",
    "            # Ensure recording is in the correct format (flatten if needed)\n",
    "            if recording.ndim > 1:\n",
    "                recording = recording.flatten()\n",
    "            \n",
    "            # Create audio input\n",
    "            audio_input = AudioInput(buffer=recording)\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipeline = VoicePipeline(\n",
    "                workflow=SingleAgentVoiceWorkflow(triage_agent),\n",
    "                config=voice_pipeline_config\n",
    "            )\n",
    "            \n",
    "            # Run the pipeline\n",
    "            # Maxim tracing is automatic via MaximOpenAIAgentsTracingProcessor\n",
    "            print(\"ü§î Processing...\")\n",
    "            result = await pipeline.run(audio_input)\n",
    "            \n",
    "            # Transfer the streamed result into chunks of audio\n",
    "            response_chunks = []\n",
    "            transcript_parts = []\n",
    "            \n",
    "            async for event in result.stream():\n",
    "                if event.type == \"voice_stream_event_audio\":\n",
    "                    response_chunks.append(event.data)\n",
    "                elif event.type == \"voice_stream_event_text\":\n",
    "                    # Capture transcript for display\n",
    "                    if hasattr(event, 'text'):\n",
    "                        transcript_parts.append(event.text)\n",
    "            \n",
    "            response_audio = np.concatenate(response_chunks, axis=0)\n",
    "            transcript = \" \".join(transcript_parts) if transcript_parts else \"Audio response generated\"\n",
    "            \n",
    "            # Play response\n",
    "            # Ensure response audio is in correct format\n",
    "            if response_audio.ndim > 1:\n",
    "                response_audio = response_audio.flatten()\n",
    "            \n",
    "            print(\"üîä Assistant is responding...\")\n",
    "            # Use the same sample rate for playback as recording\n",
    "            sd.play(response_audio, samplerate=SAMPLE_RATE)\n",
    "            sd.wait()\n",
    "            \n",
    "            print(f\"‚úÖ Response: {transcript[:100]}...\" if len(transcript) > 100 else f\"‚úÖ Response: {transcript}\")\n",
    "            print(\"üìä Interaction automatically traced to Maxim\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Cleanup\n",
    "    try:\n",
    "        logger.cleanup()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Logger cleanup error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10881677",
   "metadata": {},
   "source": [
    "## Text-based Testing (Optional)\n",
    "\n",
    "Test the agents with text input before using voice. This is useful for development and debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_agents_text():\n",
    "    \"\"\"Test agents with text input (for development/debugging).\"\"\"\n",
    "    examples = [\n",
    "        \"What's my ACME account balance? My user ID is 1234567890\",  # Account Agent test\n",
    "        \"What's trending in duck hunting gear right now?\",  # Search Agent test\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for query in examples:\n",
    "            print(f\"\\nüë§ User: {query}\")\n",
    "            # Maxim tracing is automatic via MaximOpenAIAgentsTracingProcessor\n",
    "            result = await Runner.run(triage_agent, query)\n",
    "            print(f\"ü§ñ Assistant: {result.final_output}\")\n",
    "            print(\"üìä Interaction automatically traced to Maxim\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            logger.cleanup()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Logger cleanup error: {e}\")\n",
    "\n",
    "# Uncomment to run text tests\n",
    "# await test_agents_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b9d29",
   "metadata": {},
   "source": [
    "## Run Voice Assistant\n",
    "\n",
    "Run the voice assistant to start interacting via voice. Make sure you have a microphone connected and speakers/headphones for audio output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the voice assistant\n",
    "await voice_assistant_with_maxim()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818490a6",
   "metadata": {},
   "source": [
    "## Maxim Dashboard\n",
    "\n",
    "You can view the trace of the agents' interactions on the [Maxim](https://www.getmaxim.ai) dashboard, which provides detailed insights and visualizations of the entire process, including:\n",
    "\n",
    "- Complete conversation traces\n",
    "- Audio input/output metadata\n",
    "- Agent handoffs and routing decisions\n",
    "- Tool usage and function calls\n",
    "- Performance metrics\n",
    "\n",
    "![](maxim-dashboard.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
