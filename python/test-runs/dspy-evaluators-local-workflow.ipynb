{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Maxim SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from dotenv import dotenv_values\n",
    "from maxim import Config, Maxim\n",
    "from maxim.evaluators import BaseEvaluator\n",
    "from maxim.models import (\n",
    "    LocalEvaluatorResultParameter,\n",
    "    LocalEvaluatorReturn,\n",
    "    ManualData,\n",
    "    PassFailCriteria,\n",
    "    YieldedOutput,\n",
    "    QueryBuilder\n",
    ")\n",
    "from maxim.models.evaluator import (\n",
    "    PassFailCriteriaForTestrunOverall,\n",
    "    PassFailCriteriaOnEachEntry,\n",
    ")\n",
    "import dspy\n",
    "\n",
    "config = dotenv_values()\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "WORKSPACE_ID: str = config.get(\"MAXIM_WORKSPACE_ID\") or \"\"\n",
    "WORKFLOW_ID: str = config.get(\"MAXIM_WORKFLOW_ID\") or \"\"\n",
    "DATASET_ID: str = config.get(\"MAXIM_DATASET_ID\") or \"\"\n",
    "PROMPT_VERSION_ID: str = config.get(\"MAXIM_PROMPT_VERSION_ID\") or \"\"\n",
    "MAXIM_UNKNOWN_WORKFLOW_ID: str = config.get(\"MAXIM_UNKNOWN_WORKFLOW_ID\") or \"\"\n",
    "MAXIM_INVALID_WORKFLOW_ID: str = config.get(\"MAXIM_INVALID_WORKFLOW_ID\") or \"\"\n",
    "MAXIM_PROMPT_ID: str = config.get(\"MAXIM_PROMPT_ID\") or \"\"\n",
    "\n",
    "maxim = Maxim(\n",
    "    config=Config(                \n",
    "        prompt_management=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define local workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = maxim.get_prompt(MAXIM_PROMPT_ID,\n",
    "\tQueryBuilder()\n",
    "\t.and_()\n",
    "\t.deployment_var(\"Environment\", \"prod\")\n",
    "\t.build())\n",
    "\n",
    "print(prompt.messages)\n",
    "\n",
    "def run(data: ManualData):\n",
    "    print(f\"processing => {data.get('Input')}\")\n",
    "    response = prompt.run(data.get('Input'))\n",
    "    content = response.choices[0].message.content\n",
    "    print(\"content:\", content)\n",
    "    return YieldedOutput(data=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom evaluator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature for evaluation\n",
    "class EvaluationSignature(dspy.Signature):\n",
    "    input_text = dspy.InputField(desc=\"The text output to be evaluated.\")\n",
    "    evaluation = dspy.OutputField(desc=\"Evaluation of the output for bias.\")\n",
    "\n",
    "# DSPy module using Chain-of-Thought\n",
    "class CustomEvalModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.evaluator = dspy.ChainOfThought(EvaluationSignature)\n",
    "\n",
    "    def forward(self, input_text: str):\n",
    "        result = self.evaluator(input_text=input_text)\n",
    "        return result.evaluation if hasattr(result, \"evaluation\") else \"No evaluation result.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluator that uses the DSPy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator class\n",
    "class MyCustomEvaluator(BaseEvaluator):\n",
    "    def __init__(self, pass_fail_criteria):\n",
    "        super().__init__(pass_fail_criteria=pass_fail_criteria)\n",
    "        self.custom_eval = CustomEvalModule()\n",
    "\n",
    "    def evaluate(\n",
    "        self, result: LocalEvaluatorResultParameter, data: ManualData\n",
    "    ) -> Dict[str, LocalEvaluatorReturn]:\n",
    "        # Extract the generated output\n",
    "        input_text = result.output or \"\"\n",
    "        # Run DSPy evaluation\n",
    "        evaluation_text = self.custom_eval.forward(input_text=input_text)\n",
    "        return {\n",
    "            \"DSPy Eval\": LocalEvaluatorReturn(\n",
    "                score=1,  # You can customize scoring logic\n",
    "                reasoning=evaluation_text\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and trigger test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim.create_test_run(\n",
    "    name=\"Local workflow test run from SDK\",\n",
    "    in_workspace_id=WORKSPACE_ID\n",
    ").with_data(\n",
    "    DATASET_ID\n",
    ").with_concurrency(2).with_evaluators(\n",
    "    \"Bias\",\n",
    "    MyCustomEvaluator(\n",
    "        pass_fail_criteria={\n",
    "            \"DSPy Eval\": PassFailCriteria(\n",
    "                for_testrun_overall_pass_if=PassFailCriteriaForTestrunOverall(\n",
    "                    \">\", 0, \"average\"  # Must be positional\n",
    "                ),\n",
    "                on_each_entry_pass_if=PassFailCriteriaOnEachEntry(\n",
    "                    \">\", 0  # Must be positional\n",
    "                ),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    ").yields_output(run).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
