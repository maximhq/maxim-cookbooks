{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Maxim SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from dotenv import dotenv_values\n",
    "from maxim import Config, Maxim\n",
    "from maxim.evaluators import BaseEvaluator\n",
    "from maxim.models import (\n",
    "    LocalEvaluatorResultParameter,\n",
    "    LocalEvaluatorReturn,\n",
    "    ManualData,\n",
    "    PassFailCriteria,\n",
    ")\n",
    "from maxim.models.evaluator import (\n",
    "    PassFailCriteriaForTestrunOverall,\n",
    "    PassFailCriteriaOnEachEntry,\n",
    ")\n",
    "\n",
    "config = dotenv_values()\n",
    "\n",
    "API_KEY: str = config.get(\"MAXIM_API_KEY\") or \"\"\n",
    "WORKSPACE_ID: str = config.get(\"MAXIM_WORKSPACE_ID\") or \"\"\n",
    "WORKFLOW_ID: str = config.get(\"MAXIM_WORKFLOW_ID\") or \"\"\n",
    "DATASET_ID: str = config.get(\"MAXIM_DATASET_ID\") or \"\"\n",
    "PROMPT_VERSION_ID: str = config.get(\"MAXIM_PROMPT_VERSION_ID\") or \"\"\n",
    "MAXIM_UNKNOWN_WORKFLOW_ID: str = config.get(\"MAXIM_UNKNOWN_WORKFLOW_ID\") or \"\"\n",
    "MAXIM_INVALID_WORKFLOW_ID: str = config.get(\"MAXIM_INVALID_WORKFLOW_ID\") or \"\"\n",
    "\n",
    "maxim = Maxim(\n",
    "    config=Config(\n",
    "        api_key=API_KEY,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyCustomEvaluator(BaseEvaluator):\n",
    "    \"\"\"\n",
    "    Custom evaluator class that extends BaseEvaluator to perform custom evaluations.\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate(\n",
    "        self, result: LocalEvaluatorResultParameter, data: ManualData\n",
    "    ) -> Dict[str, LocalEvaluatorReturn]:\n",
    "        \"\"\"\n",
    "        You are supposed to override this function and run evaluations\n",
    "        Args:\n",
    "            result (LocalEvaluatorResultParameter): The result parameter containing evaluation data\n",
    "            data (ManualData): The manual data to evaluate against\n",
    "        Returns:\n",
    "            Dict[str, LocalEvaluatorReturn]: Dictionary mapping evaluation names to score results.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Evaluation 1\": LocalEvaluatorReturn(score=1),\n",
    "            \"Evaluation 2\": LocalEvaluatorReturn(score=False, reasoning=\"Just chillll\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and trigger test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating test run config...\n",
      "Sanitizing data...\n",
      "Sanitizing evaluators...\n",
      "Verifying if Bias is added to the workspace..\n",
      "Creating test run: Local evaluators from SDK\n",
      "You can view your test run here: https://app.getmaxim.ai/workspace/cln4nw1n80000mc3wqqwk4j0z/testrun/cm8eomjwi00nrycsfancrsu7h\n",
      "You can safely quit this session or wait to see the final output in console.\n",
      "Waiting for test run to complete...\n",
      "Polling interval: 15 seconds\n",
      "┌───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                                Test run status: QUEUED                                                │\n",
      "├───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ totalEntries: 0 | runningEntries: 0 | queuedEntries: 0 | failedEntries: 0 | completedEntries: 0 | stoppedEntries: 0   │\n",
      "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                                Test run status: RUNNING                                               │\n",
      "├───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ totalEntries: 4 | runningEntries: 3 | queuedEntries: 0 | failedEntries: 0 | completedEntries: 1 | stoppedEntries: 0   │\n",
      "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                               Test run status: COMPLETE                                               │\n",
      "├───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ totalEntries: 4 | runningEntries: 0 | queuedEntries: 0 | failedEntries: 0 | completedEntries: 4 | stoppedEntries: 0   │\n",
      "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "All entries processed. Test run completed.\n",
      "Test run \"Local evaluators from SDK\" completed successfully!🎉 \n",
      "View the report here: https://app.getmaxim.ai/workspace/cln4nw1n80000mc3wqqwk4j0z/testrun/cm8eomjwi00nrycsfancrsu7h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RunResult(test_run_result=TestRunResult(link='https://app.getmaxim.ai/workspace/cln4nw1n80000mc3wqqwk4j0z/testrun/cm8eomjwi00nrycsfancrsu7h', result=[TestRunResultObj(name='Local evaluators from SDK', individual_evaluator_mean_score={'Bias': EvaluatorMeanScore(score=0, out_of=1, is_pass=True), 'Evaluation 1': EvaluatorMeanScore(score=1, out_of=None, is_pass=False), 'Evaluation 2': EvaluatorMeanScore(score=0, out_of=None, is_pass=True)}, usage=TestRunTokenUsage(total=None, input=None, completion=None), cost=TestRunCost(total=0, input=0, completion=0), latency=TestRunLatency(min=3586, max=12031.6804, p50=7299, p90=12031, p95=12031, p99=12031, mean=8003.25, standard_deviation=3057.8634, total=4))]), failed_entry_indices=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxim.create_test_run(\n",
    "    name=\"Local evaluators from SDK\", in_workspace_id=WORKSPACE_ID\n",
    ").with_data(DATASET_ID).with_concurrency(2).with_evaluators(\n",
    "    \"Bias\",\n",
    "    MyCustomEvaluator(\n",
    "        pass_fail_criteria={\n",
    "            \"Evaluation 1\": PassFailCriteria(\n",
    "                for_testrun_overall_pass_if=PassFailCriteriaForTestrunOverall(\n",
    "                    \">\", 3, \"average\"\n",
    "                ),\n",
    "                on_each_entry_pass_if=PassFailCriteriaOnEachEntry(\">\", 1),\n",
    "            ),\n",
    "            \"Evaluation 2\": PassFailCriteria(\n",
    "                for_testrun_overall_pass_if=PassFailCriteriaForTestrunOverall(\n",
    "                    overall_should_be=\"!=\", value=2, for_result=\"average\"\n",
    "                ),\n",
    "                on_each_entry_pass_if=PassFailCriteriaOnEachEntry(\"=\", True),\n",
    "            ),\n",
    "        }\n",
    "    ),\n",
    ").with_workflow_id(\n",
    "    WORKFLOW_ID\n",
    ").run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
