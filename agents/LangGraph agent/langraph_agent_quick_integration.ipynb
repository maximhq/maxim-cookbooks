{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0f60da",
   "metadata": {},
   "source": [
    "First setting up the environment with following keys\n",
    "\n",
    "```\n",
    "ANTHROPIC_API_KEY=\n",
    "OPENAI_API_KEY=\n",
    "MAXIM_API_KEY=\n",
    "LOG_REPO_ID=\n",
    "TAVILY_API_KEY=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75002adc",
   "metadata": {},
   "source": [
    "The original source code of this example is here: https://github.com/langchain-ai/langgraph-example/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b82cf8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"LOG_REPO_ID\")\n",
    "_set_env(\"MAXIM_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2194ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import operator\n",
    "import os\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from typing import Annotated, List, Literal, Sequence, Tuple, TypedDict, Union\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_anthropic import Anthropic, ChatAnthropic\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph, add_messages\n",
    "from langgraph.prebuilt import ToolNode, create_react_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic.type_adapter import R\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "tavilyApiKey=os.environ.get(\"TAVILY_API_KEY\")\n",
    "anthropicApiKey=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "openAIKey=os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1, tavily_api_key=tavilyApiKey)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def _get_model(model_name: str):\n",
    "    if model_name == \"openai\":\n",
    "        model = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", api_key=openAIKey)\n",
    "    elif model_name == \"anthropic\":\n",
    "        model = ChatAnthropic(\n",
    "            temperature=0,\n",
    "            model_name=\"claude-3-sonnet-20240229\",\n",
    "            api_key=anthropicApiKey,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    model = model.bind_tools(tools)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Be a helpful assistant\"\"\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state, config):\n",
    "    messages = state[\"messages\"]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "    model_name = config.get(\"configurable\", {}).get(\"model_name\", \"anthropic\")\n",
    "    model = _get_model(model_name)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "\n",
    "# Define the config\n",
    "class GraphConfig(TypedDict):\n",
    "    model_name: Literal[\"anthropic\", \"openai\"]\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState, config_schema=GraphConfig)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "322fb60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, the last 3 captains of the Indian cricket team in ODIs are:\n",
      "\n",
      "1. Rohit Sharma (current captain)\n",
      "2. Virat Kohli \n",
      "3. MS Dhoni\n",
      "\n",
      "The key details:\n",
      "\n",
      "- Rohit Sharma has captained India in 45 ODIs so far, winning 34 and losing 10 with a win percentage of 75.55%.\n",
      "\n",
      "- Virat Kohli captained India in ODIs before Rohit took over. His captaincy record is not mentioned.\n",
      "\n",
      "- MS Dhoni captained India in 72 T20Is and 49 Tests, achieving good success rates. His ODI captaincy record is not explicitly stated but he was the captain before Kohli.\n",
      "\n",
      "So the last 3 ODI captains in chronological order are MS Dhoni, Virat Kohli and the current captain Rohit Sharma.\n"
     ]
    }
   ],
   "source": [
    "config = {\"recursion_limit\": 50, \"callbacks\": []}\n",
    "response = \"\"\n",
    "async for event in app.astream(input={\"messages\": [\"list last 3 captains of indian cricket team in ODI\"]}, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k == \"agent\":\n",
    "            response = str(v[\"messages\"][0].content)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
